{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish importing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import errno\n",
    "import glob\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, accuracy_score\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "warnings.filterwarnings('ignore')\n",
    "print('finish importing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def import2dArray(file_name, file_type=\"f\", return_sparse=False):\n",
    "    if file_name[-4:] == \".npz\":\n",
    "        print(\"Loading sparse array\")\n",
    "        array = sp.load_npz(file_name)\n",
    "        if return_sparse is False:\n",
    "            array = array.toarray()\n",
    "    elif file_name[-4:] == \".npy\":\n",
    "        print(\"Loading numpy array\")\n",
    "        array = np.load(file_name)#\n",
    "    else:\n",
    "        with open(file_name, \"r\") as infile:\n",
    "            if file_type == \"i\":\n",
    "                array = [list(map(int, line.strip().split())) for line in infile]\n",
    "            elif file_type == \"f\":\n",
    "                array = [list(map(float, line.strip().split())) for line in infile]\n",
    "            elif file_type == \"discrete\":\n",
    "                array = [list(line.strip().split()) for line in infile]\n",
    "                for dv in array:\n",
    "                    for v in range(len(dv)):\n",
    "                        dv[v] = int(dv[v][:-1])\n",
    "            else:\n",
    "                array = np.asarray([list(line.strip().split()) for line in infile])\n",
    "        array = np.asarray(array)\n",
    "    print(\"successful import\", file_name)\n",
    "    return array\n",
    "\n",
    "\n",
    "def write1dArray(array, name):\n",
    "    try:\n",
    "        file = open(name, \"w\")\n",
    "        print(\"starting array\")\n",
    "        for i in range(len(array)):\n",
    "            file.write(str(array[i]))\n",
    "            file.write(\"\\n\")\n",
    "        file.close()\n",
    "    except FileNotFoundError:\n",
    "        print(\"FAILURE\")\n",
    "   \n",
    "\n",
    "    print(\"successful write\", name)\n",
    "    \n",
    "    \n",
    "def write2dArray(array, name):\n",
    "    try:\n",
    "        file = open(name, \"w\")\n",
    "        print(\"starting array\")\n",
    "        for i in range(len(array)):\n",
    "            for n in range(len(array[i])):\n",
    "                file.write(str(array[i][n]) + \" \")\n",
    "            file.write(\"\\n\")\n",
    "        file.close()\n",
    "    except FileNotFoundError:\n",
    "        print(\"FAILURE\")\n",
    "    \n",
    "\n",
    "    print(\"successful write\", name)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful import /home/rana/mymodel4/building/coaccuranceForMoe\n",
      "successful import /home/rana/mymodel4/building/features3_Glove50d300.txt\n",
      "(9368, 300)\n",
      "finished : 0\n",
      "finished : 1000\n",
      "finished : 2000\n",
      "finished : 3000\n",
      "finish_Finally\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[138]:\n",
    "\n",
    "#Reading from files \n",
    "##############################################################################\n",
    "##############################################################################\n",
    "base_folder_results=\"//media/rana/2TB/MoECode/results/\"\n",
    "\n",
    "base_folder=\"/home/rana/mymodel4/building/\"\n",
    "#base_folder2=\"/home/rana/mymodel4/movies1/Tom/\"\n",
    "entity_file=base_folder+\"final_ent.txt\"\n",
    "entityID_file=base_folder+\"sorted_ent_wikiID\"\n",
    "\n",
    "feature_file=base_folder+\"features3_that_have_Glove50d300.txt\"\n",
    "\n",
    "Wj_file=base_folder+\"features3_Glove50d300.txt\"\n",
    "Co_file=base_folder+\"BoWFinalReclean/\"\n",
    "ent=[]\n",
    "fea=[]\n",
    "idf=[]\n",
    "with open(entity_file,\"r\") as f:\n",
    "    for line in f:\n",
    "        key= line.strip()\n",
    "        ent.append(key)\n",
    "with open(feature_file,\"r\")as f:\n",
    "    for line in f:\n",
    "        key=line.strip()\n",
    "        fea.append(key)\n",
    "\n",
    "\n",
    "entities=dict(zip(ent,range(len(ent))))\n",
    "features=dict(zip(fea,range(len(fea))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cooccurance=import2dArray(base_folder+'coaccuranceForMoe')#[number of words X number of entities]\n",
    "co=np.transpose(cooccurance)\n",
    "\n",
    "\n",
    "\n",
    "gloveVectors=[]\n",
    "gloveVectors= import2dArray(Wj_file)\n",
    "gloveVectors =np.array(gloveVectors)       \n",
    "\n",
    "print(gloveVectors.shape)\n",
    "\n",
    "\n",
    "#   create the dictionary    \n",
    "cooccurrence_counts = defaultdict(float)\n",
    "\n",
    "for i in range(0,len(ent)):\n",
    "    countp=0\n",
    "    countn=0\n",
    "    if i%1000==0:\n",
    "       \n",
    "        print('finished :',i)\n",
    "    for j in range(0,len(fea)):\n",
    "        if(co[i][j])>0:\n",
    "            cooccurrence_counts[(i, j)]=co[i][j]\n",
    "        elif (co[i][j])==0:# and countn<1000:#<2000 is for negative sampling\n",
    "            countn=countn+1\n",
    "            cooccurrence_counts[(i, j)]=1.0\n",
    "                    \n",
    "cooccurrence_matrix = {\n",
    "            (words[0], words[1]): count\n",
    "            for words, count in cooccurrence_counts.items() if count>=0}\n",
    "print('finish_Finally')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25588"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del co,cooccurrence_counts,cooccurance\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe and GloVe MoE implementation (standard and  weighted version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[4]:\n",
    "# we used the implementation in https://github.com/GradySimon/tensorflow-glove\n",
    "\n",
    "class NotTrainedError(Exception):\n",
    "    pass\n",
    "\n",
    "class NotFitToCorpusError(Exception):\n",
    "    pass\n",
    "\n",
    "class GloVeModel():\n",
    "    def __init__(self, count_,entities,fea,embedding_size, entities_size, max_vocab_size=100000, min_occurrences=0,\n",
    "                 scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05):\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.max_vocab_size = entities_size#max_vocab_size\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.cooccurrence_cap = cooccurrence_cap\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.mds= mds\n",
    "\n",
    "        self.__words = fea\n",
    "        self._entities=entities\n",
    "        self.entities_size=len(entities)\n",
    "        self.__word_to_id = [i for i in range(0,len(fea))]\n",
    "        self.__cooccurrence_matrix = count_\n",
    "        self.__embeddings = None\n",
    "        self.__embeddings_en = None\n",
    "       \n",
    "    def fit_to_corpus(self, count_):\n",
    "\n",
    "        self.__build_graph()\n",
    "\n",
    "   \n",
    "    def __build_graph(self):\n",
    "        self.__graph = tf.Graph()\n",
    "        with self.__graph.as_default(), self.__graph.device(_device_for_node):\n",
    "            count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,\n",
    "                                    name='max_cooccurrence_count')\n",
    "            scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,\n",
    "                                         name=\"scaling_factor\")\n",
    "\n",
    "            self.__focal_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                name=\"focal_words\")\n",
    "            self.__context_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                  name=\"context_words\")\n",
    "            self.__cooccurrence_count = tf.placeholder(tf.float32, shape=[self.batch_size],\n",
    "                                           name=\"cooccurrence_count\")\n",
    "\n",
    "            self.__lamda=tf.placeholder(dtype=tf.float32)\n",
    "            focal_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"focal_embeddings\")\n",
    "            context_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.entities_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"context_embeddings\")\n",
    "            \n",
    "\n",
    "            focal_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                       name='focal_biases')\n",
    "            context_biases = tf.Variable(tf.random_uniform([self.entities_size], 1.0, -1.0),\n",
    "                                         name=\"context_biases\")\n",
    "\n",
    "            focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.__focal_input)\n",
    "            \n",
    "            #WEPrime=tf.constant(mds)\n",
    "            #context_embedding = tf.nn.embedding_lookup([WEPrime], self.__context_input)\n",
    "            context_embedding = tf.nn.embedding_lookup([context_embeddings], self.__context_input)\n",
    "            focal_bias = tf.nn.embedding_lookup([focal_biases], self.__focal_input)\n",
    "            context_bias = tf.nn.embedding_lookup([context_biases], self.__context_input)\n",
    "\n",
    "            weighting_factor = tf.minimum(\n",
    "                1.0,\n",
    "                tf.pow(\n",
    "                    tf.div(self.__cooccurrence_count, count_max),\n",
    "                    scaling_factor))\n",
    "\n",
    "            embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "            #tf.log(tf.to_float(self.__cooccurrence_count))\n",
    "            log_cooccurrences =tf.log (tf.to_float(self.__cooccurrence_count))\n",
    "\n",
    "            distance_expr = tf.square(tf.add_n([\n",
    "                embedding_product,\n",
    "                focal_bias,\n",
    "                context_bias,\n",
    "                tf.negative(log_cooccurrences)]))\n",
    "            \n",
    "            single_losses =tf.multiply(weighting_factor, distance_expr)\n",
    "            print('single_lose',single_losses.shape)\n",
    "            #WEPrime=tf.constant(mds)\n",
    "\n",
    "            #self.__squareEqludian=self.__lamda*(tf.reduce_sum(tf.square(tf.subtract(context_embeddings, WEPrime))))\n",
    "\n",
    "            self.__total_loss = tf.reduce_sum(single_losses)#+self.__squareEqludian\n",
    "            tf.summary.scalar(\"GloVe_loss\", self.__total_loss)\n",
    "            self.__optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(\n",
    "                self.__total_loss)\n",
    "            self.__summary = tf.summary.merge_all()\n",
    "\n",
    "            self.__combined_embeddings = focal_embeddings\n",
    "            self.__entities_embeddings = context_embeddings\n",
    "            self.__single_losses = single_losses\n",
    "            self.__context_biases = context_biases\n",
    "            self.__focal_biases = focal_biases\n",
    "    def train(self, num_epochs, log_dir=None, summary_batch_interval=1000,\n",
    "              tsne_epoch_interval=None):\n",
    "        should_write_summaries = log_dir is not None and summary_batch_interval\n",
    "        should_generate_tsne = log_dir is not None and tsne_epoch_interval\n",
    "        batches = self.__prepare_batches()\n",
    "        total_steps = 0\n",
    "        with tf.Session(graph=self.__graph) as session:\n",
    "            if should_write_summaries:\n",
    "                print(\"Writing TensorBoard summaries to {}\".format(log_dir))\n",
    "                summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)\n",
    "            tf.global_variables_initializer().run()\n",
    "            count=0\n",
    "            self.__losses=None\n",
    "            allLoses=[]\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                loss_epoch=0\n",
    "                allError=np.zeros((self.entities_size),dtype=np.float)\n",
    "\n",
    "                count=count+1\n",
    "                print('epoch:', count)\n",
    "                shuffle(batches)\n",
    "                #print('finish shuff')\n",
    "                for batch_index, batch in enumerate(batches):\n",
    "                    i_s, j_s, counts = batch\n",
    "                    #print('servive batches')\n",
    "                    if batch_index==len(batches)-1:\n",
    "                        lamda=1000.0\n",
    "                    else:\n",
    "                        lamda=0.0\n",
    "                    if len(counts) != self.batch_size:\n",
    "                        continue\n",
    "                    feed_dict = {\n",
    "                        self.__focal_input: i_s,\n",
    "                        self.__context_input: j_s,\n",
    "                        self.__cooccurrence_count: counts,\n",
    "                        self.__lamda:lamda}\n",
    "                    _,loss,ErrorEst=session.run([self.__optimizer,self.__total_loss,self.__single_losses], feed_dict=feed_dict)\n",
    "                    loss_epoch=loss_epoch+loss\n",
    "                    for i in range(len(j_s)):\n",
    "                        allError[j_s[i]]=allError[j_s[i]]+(ErrorEst)[i]\n",
    "                    \n",
    "                    #print(loss,' ')\n",
    "                    if should_write_summaries and (total_steps + 1) % summary_batch_interval == 0:\n",
    "                        summary_str = session.run(self.__summary, feed_dict=feed_dict)\n",
    "                        summary_writer.add_summary(summary_str, total_steps)\n",
    "                    total_steps += 1\n",
    "                if should_generate_tsne and (epoch + 1) % tsne_epoch_interval == 0:\n",
    "                    current_embeddings = self.__combined_embeddings.eval()\n",
    "                    output_path = os.path.join(log_dir, \"epoch{:03d}.png\".format(epoch + 1))\n",
    "                    self.generate_tsne(output_path, embeddings=current_embeddings)\n",
    "                    \n",
    "                allLoses.append(loss_epoch)\n",
    "                \n",
    "                print('self.__total_loss',loss_epoch)\n",
    "            del(batch_index)\n",
    "            del(i_s)\n",
    "            del(j_s)\n",
    "            del(counts)\n",
    "            gc.collect()\n",
    "            self.__losses=allLoses\n",
    "            self.__single_lossesAll=allError\n",
    "            self.__embeddings = self.__combined_embeddings.eval()\n",
    "            self.__embeddings_en = self.__entities_embeddings.eval()\n",
    "            self.__context_biasesE=self.__context_biases.eval()\n",
    "            self.__focal_biasesW=self.__focal_biases.eval()\n",
    "            del(batches)\n",
    "            gc.collect()\n",
    "            if should_write_summaries:\n",
    "                summary_writer.close()\n",
    "            \n",
    "           \n",
    "   \n",
    "                \n",
    "                \n",
    "    def rana (self,f):\n",
    "        print(self.entities_size)\n",
    "        \n",
    "    def embedding_for(self, word_str_or_id):\n",
    "        if isinstance(word_str_or_id, str):\n",
    "            return self.embeddings[self.__word_to_id[word_str_or_id]]\n",
    "        elif isinstance(word_str_or_id, int):\n",
    "            return self.embeddings[word_str_or_id]\n",
    "    def __prepare_batches(self):\n",
    "        if self.__cooccurrence_matrix is None:\n",
    "            raise NotFitToCorpusError(\n",
    "                \"Need to fit model to corpus before preparing training batches.\")\n",
    "        cooccurrences = [(word_ids[0], word_ids[1], count)\n",
    "                         for word_ids, count in self.__cooccurrence_matrix.items()]\n",
    "        i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "        return list(_batchify(self.batch_size, i_indices, j_indices, counts))\n",
    "\n",
    " \n",
    "    def embedding_for_ent(self, word_str_or_id):\n",
    "       \n",
    "        return self.__entities_embeddings[word_str_or_id]\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.__words)\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if self.__words is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before accessing words.\")\n",
    "        return self.__words\n",
    "    @property\n",
    "    def loss(self):\n",
    "        if self.__losses is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__losses\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        if self.__embeddings is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings\n",
    "    @property\n",
    "    def embeddings_en(self):\n",
    "        if self.__embeddings_en is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings_en\n",
    "    @property\n",
    "    def errorEstimation(self):\n",
    "        if self.__single_lossesAll is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__single_lossesAll\n",
    "    @property\n",
    "    def context_biasesE(self):\n",
    "        if self.__context_biasesE is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__context_biasesE\n",
    "    @property\n",
    "    def focal_biasesW(self):\n",
    "        if self.__focal_biasesW is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__focal_biasesW\n",
    "    def id_for_word(self, word):\n",
    "        if self.__word_to_id is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before looking up word ids.\")\n",
    "        return self.__word_to_id[word]\n",
    "\n",
    "    def generate_tsne(self, path=None, size=(100, 100), word_count=1000, embeddings=None):\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embeddings\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "        low_dim_embs = tsne.fit_transform(embeddings[:word_count, :])\n",
    "        labels = self.words[:word_count]\n",
    "        return _plot_with_labels(low_dim_embs, labels, path, size)\n",
    "\n",
    "\n",
    "def _context_windows(region, left_size, right_size):\n",
    "    for i, word in enumerate(region):\n",
    "        start_index = i - left_size\n",
    "        end_index = i + right_size\n",
    "        left_context = _window(region, start_index, i - 1)\n",
    "        right_context = _window(region, i + 1, end_index)\n",
    "        yield (left_context, word, right_context)\n",
    "\n",
    "\n",
    "def _window(region, start_index, end_index):\n",
    "    \"\"\"\n",
    "    Returns the list of words starting from `start_index`, going to `end_index`\n",
    "    taken from region. If `start_index` is a negative number, or if `end_index`\n",
    "    is greater than the index of the last word in region, this function will pad\n",
    "    its return value with `NULL_WORD`.\n",
    "    \"\"\"\n",
    "    last_index = len(region) + 1\n",
    "    selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]\n",
    "    return selected_tokens\n",
    "\n",
    "\n",
    "def _device_for_node(n):\n",
    "    if n.type == \"MatMul\":\n",
    "        return \"/cpu:0\"\n",
    "    else:\n",
    "        return \"/cpu:0\"\n",
    "\n",
    "\n",
    "def _batchify(batch_size, *sequences):\n",
    "    for i in range(0, len(sequences[0]), batch_size):\n",
    "        yield tuple(sequence[i:i+batch_size] for sequence in sequences)\n",
    "\n",
    "\n",
    "def _plot_with_labels(low_dim_embs, labels, path, size):\n",
    "    import matplotlib.pyplot as plt\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    figure = plt.figure(figsize=size)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right',\n",
    "                     va='bottom')\n",
    "    if path is not None:\n",
    "        figure.savefig(path)\n",
    "        plt.close(figure)\n",
    "\n",
    "\n",
    "# In[245]:\n",
    "\n",
    "\n",
    "class GloVeModelMOE():\n",
    "    def __init__(self, count_,entities,fea,g_K_j,empEnt1,empFea1, entbias1,feabias1,embedding_size, entities_size,max_vocab_size=100000, min_occurrences=0,\n",
    "                 scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05):\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.max_vocab_size = entities_size#max_vocab_size\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.cooccurrence_cap = cooccurrence_cap\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.mds= mds\n",
    "\n",
    "        self.__words = fea\n",
    "        self._entities=entities\n",
    "        print('len(entities)',len(entities))\n",
    "        self.entities_size=len(entities)\n",
    "        self.__word_to_id = [i for i in range(0,len(fea))]\n",
    "        self.__cooccurrence_matrix = count_\n",
    "        self.__embeddings = None\n",
    "        self.__embeddings_en = None\n",
    "        self.empEnt1=empEnt1\n",
    "        self.empFea1=empFea1\n",
    "        self.entbias1=entbias1\n",
    "        self.feabias1=feabias1\n",
    "        self.g_K_j=g_K_j\n",
    "    def fit_to_corpus(self, count_):\n",
    "\n",
    "        self.__build_graph()\n",
    "\n",
    "   \n",
    "    def __build_graph(self):\n",
    "        self.__graph = tf.Graph()\n",
    "        with self.__graph.as_default(), self.__graph.device(_device_for_node):\n",
    "            count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,\n",
    "                                    name='max_cooccurrence_count')\n",
    "            scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,\n",
    "                                         name=\"scaling_factor\")\n",
    "\n",
    "            self.__focal_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                name=\"focal_words\")\n",
    "            self.__context_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                  name=\"context_words\")\n",
    "            self.__cooccurrence_count = tf.placeholder(tf.float32, shape=[self.batch_size],\n",
    "                                           name=\"cooccurrence_count\")\n",
    "\n",
    "            self.__lamda=tf.placeholder(dtype=tf.float32)\n",
    "            focal_embeddings = tf.Variable(\n",
    "                self.empFea1,\n",
    "                name=\"focal_embeddings\")\n",
    "            context_embeddings = tf.Variable(\n",
    "                self.empEnt1,\n",
    "                name=\"context_embeddings\")\n",
    "            \n",
    "\n",
    "            focal_biases = tf.Variable(self.feabias1,\n",
    "                                       name='focal_biases')\n",
    "            context_biases = tf.Variable(self.entbias1,\n",
    "                                         name=\"context_biases\")\n",
    "\n",
    "            focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.__focal_input)\n",
    "            # the probability from the gating network\n",
    "            G_K_J = tf.constant(self.g_K_j)\n",
    "            G_K_J_weight = tf.nn.embedding_lookup([G_K_J], self.__context_input)\n",
    "            print(G_K_J_weight.shape)\n",
    "            context_embedding = tf.nn.embedding_lookup([context_embeddings], self.__context_input)\n",
    "            focal_bias = tf.nn.embedding_lookup([focal_biases], self.__focal_input)\n",
    "            context_bias = tf.nn.embedding_lookup([context_biases], self.__context_input)\n",
    "            #not used for simplicity\n",
    "            weighting_factor = tf.minimum(\n",
    "                1.0,\n",
    "                tf.pow(\n",
    "                    tf.div(self.__cooccurrence_count, count_max),\n",
    "                    scaling_factor))\n",
    "\n",
    "            embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "            #tf.log(tf.to_float(self.__cooccurrence_count))\n",
    "            log_cooccurrences = tf.log (tf.to_float(self.__cooccurrence_count))\n",
    "\n",
    "            distance_expr = tf.square(tf.add_n([\n",
    "                embedding_product,\n",
    "                focal_bias,\n",
    "                context_bias,\n",
    "                tf.negative(log_cooccurrences)]))\n",
    "            single_losses1 = tf.multiply(weighting_factor, distance_expr)\n",
    "            single_losses = tf.multiply(G_K_J_weight, distance_expr)\n",
    "            print('single_lose',single_losses.shape)\n",
    "            #WEPrime=tf.constant(mds)\n",
    "\n",
    "            #self.__squareEqludian=self.__lamda*(tf.reduce_sum(tf.square(tf.subtract(context_embeddings, WEPrime))))\n",
    "\n",
    "            self.__total_loss = tf.reduce_sum(single_losses)#+self.__squareEqludian\n",
    "            tf.summary.scalar(\"GloVe_loss\", self.__total_loss)\n",
    "            self.__optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(\n",
    "                self.__total_loss)\n",
    "            self.__summary = tf.summary.merge_all()\n",
    "\n",
    "            self.__combined_embeddings = focal_embeddings\n",
    "            self.__entities_embeddings = context_embeddings\n",
    "            self.__single_losses = single_losses1\n",
    "            self.__context_biases = context_biases\n",
    "            self.__focal_biases = focal_biases\n",
    "    def train(self, num_epochs, log_dir=None, summary_batch_interval=1000,\n",
    "              tsne_epoch_interval=None):\n",
    "        should_write_summaries = log_dir is not None and summary_batch_interval\n",
    "        should_generate_tsne = log_dir is not None and tsne_epoch_interval\n",
    "        batches = self.__prepare_batches()\n",
    "        total_steps = 0\n",
    "        with tf.Session(graph=self.__graph) as session:\n",
    "            if should_write_summaries:\n",
    "                print(\"Writing TensorBoard summaries to {}\".format(log_dir))\n",
    "                summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)\n",
    "            tf.global_variables_initializer().run()\n",
    "            count=0\n",
    "            self.__losses=None\n",
    "            allLoses=[]\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                loss_epoch=0\n",
    "                allError=np.zeros((self.entities_size),dtype=np.float)\n",
    "\n",
    "                count=count+1\n",
    "                print('epoch:', count)\n",
    "                shuffle(batches)\n",
    "                #print('finish shuff')\n",
    "                for batch_index, batch in enumerate(batches):\n",
    "                    i_s, j_s, counts = batch\n",
    "                    #print('servive batches')\n",
    "                    if batch_index==len(batches)-1:\n",
    "                        lamda=1000.0\n",
    "                    else:\n",
    "                        lamda=0.0\n",
    "                    if len(counts) != self.batch_size:\n",
    "                        continue\n",
    "                    feed_dict = {\n",
    "                        self.__focal_input: i_s,\n",
    "                        self.__context_input: j_s,\n",
    "                        self.__cooccurrence_count: counts,\n",
    "                        self.__lamda:lamda}\n",
    "                    _,loss,ErrorEst=session.run([self.__optimizer,self.__total_loss,self.__single_losses], feed_dict=feed_dict)\n",
    "                    loss_epoch=loss_epoch+loss\n",
    "                    for i in range(len(j_s)):\n",
    "                        allError[j_s[i]]=allError[j_s[i]]+(ErrorEst)[i]\n",
    "                    \n",
    "                    #print(loss,' ')\n",
    "                    if should_write_summaries and (total_steps + 1) % summary_batch_interval == 0:\n",
    "                        summary_str = session.run(self.__summary, feed_dict=feed_dict)\n",
    "                        summary_writer.add_summary(summary_str, total_steps)\n",
    "                    total_steps += 1\n",
    "                if should_generate_tsne and (epoch + 1) % tsne_epoch_interval == 0:\n",
    "                    current_embeddings = self.__combined_embeddings.eval()\n",
    "                    output_path = os.path.join(log_dir, \"epoch{:03d}.png\".format(epoch + 1))\n",
    "                    self.generate_tsne(output_path, embeddings=current_embeddings)\n",
    "                    \n",
    "                allLoses.append(loss_epoch)\n",
    "                \n",
    "                print('self.__total_loss',loss_epoch)\n",
    "            del(batch_index)\n",
    "            del(i_s)\n",
    "            del(j_s)\n",
    "            del(counts)\n",
    "            gc.collect()\n",
    "            self.__losses=allLoses\n",
    "            self.__single_lossesAll=allError\n",
    "            self.__embeddings = self.__combined_embeddings.eval()\n",
    "            self.__embeddings_en = self.__entities_embeddings.eval()\n",
    "            self.__context_biasesE=self.__context_biases.eval()\n",
    "            self.__focal_biasesW=self.__focal_biases.eval()\n",
    "            del(batches)\n",
    "            gc.collect()\n",
    "            if should_write_summaries:\n",
    "                summary_writer.close()\n",
    "            \n",
    "            ''' #del(batches)\n",
    "            del(batch_index)\n",
    "            del(i_s)\n",
    "            del(j_s)\n",
    "            del(counts)\n",
    "            del(feed_dict)\n",
    "            gc.collect()    '''\n",
    "             \n",
    "   \n",
    "                \n",
    "                \n",
    "    def rana (self,f):\n",
    "        print(self.entities_size)\n",
    "        \n",
    "    def embedding_for(self, word_str_or_id):\n",
    "        if isinstance(word_str_or_id, str):\n",
    "            return self.embeddings[self.__word_to_id[word_str_or_id]]\n",
    "        elif isinstance(word_str_or_id, int):\n",
    "            return self.embeddings[word_str_or_id]\n",
    "    def __prepare_batches(self):\n",
    "        if self.__cooccurrence_matrix is None:\n",
    "            raise NotFitToCorpusError(\n",
    "                \"Need to fit model to corpus before preparing training batches.\")\n",
    "        cooccurrences = [(word_ids[0], word_ids[1], count)\n",
    "                         for word_ids, count in self.__cooccurrence_matrix.items()]\n",
    "        i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "        return list(_batchify(self.batch_size, i_indices, j_indices, counts))\n",
    "\n",
    " \n",
    "    def embedding_for_ent(self, word_str_or_id):\n",
    "       \n",
    "        return self.__entities_embeddings[word_str_or_id]\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.__words)\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if self.__words is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before accessing words.\")\n",
    "        return self.__words\n",
    "    @property\n",
    "    def loss(self):\n",
    "        if self.__losses is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__losses\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        if self.__embeddings is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings\n",
    "    @property\n",
    "    def embeddings_en(self):\n",
    "        if self.__embeddings_en is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings_en\n",
    "    @property\n",
    "    def errorEstimation(self):\n",
    "        if self.__single_lossesAll is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__single_lossesAll\n",
    "    @property\n",
    "    def context_biasesE(self):\n",
    "        if self.__context_biasesE is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__context_biasesE\n",
    "    @property\n",
    "    def focal_biasesW(self):\n",
    "        if self.__focal_biasesW is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__focal_biasesW\n",
    "    def id_for_word(self, word):\n",
    "        if self.__word_to_id is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before looking up word ids.\")\n",
    "        return self.__word_to_id[word]\n",
    "\n",
    "    def generate_tsne(self, path=None, size=(100, 100), word_count=1000, embeddings=None):\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embeddings\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "        low_dim_embs = tsne.fit_transform(embeddings[:word_count, :])\n",
    "        labels = self.words[:word_count]\n",
    "        return _plot_with_labels(low_dim_embs, labels, path, size)\n",
    "\n",
    "\n",
    "def _context_windows(region, left_size, right_size):\n",
    "    for i, word in enumerate(region):\n",
    "        start_index = i - left_size\n",
    "        end_index = i + right_size\n",
    "        left_context = _window(region, start_index, i - 1)\n",
    "        right_context = _window(region, i + 1, end_index)\n",
    "        yield (left_context, word, right_context)\n",
    "\n",
    "\n",
    "def _window(region, start_index, end_index):\n",
    "    \"\"\"\n",
    "    Returns the list of words starting from `start_index`, going to `end_index`\n",
    "    taken from region. If `start_index` is a negative number, or if `end_index`\n",
    "    is greater than the index of the last word in region, this function will pad\n",
    "    its return value with `NULL_WORD`.\n",
    "    \"\"\"\n",
    "    last_index = len(region) + 1\n",
    "    selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]\n",
    "    return selected_tokens\n",
    "\n",
    "\n",
    "def _device_for_node(n):\n",
    "    if n.type == \"MatMul\":\n",
    "        return \"/cpu:0\"\n",
    "    else:\n",
    "        return \"/cpu:0\"\n",
    "\n",
    "\n",
    "def _batchify(batch_size, *sequences):\n",
    "    for i in range(0, len(sequences[0]), batch_size):\n",
    "        yield tuple(sequence[i:i+batch_size] for sequence in sequences)\n",
    "\n",
    "\n",
    "def _plot_with_labels(low_dim_embs, labels, path, size):\n",
    "    import matplotlib.pyplot as plt\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    figure = plt.figure(figsize=size)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right',\n",
    "                     va='bottom')\n",
    "    if path is not None:\n",
    "        figure.savefig(path)\n",
    "        plt.close(figure)\n",
    "\n",
    "\n",
    "# In[137]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want to train Glove for all the entitie to compare it with our model\n",
    "##############################################################################################\n",
    "model=GloVeModel(cooccurrence_matrix,fea,ent,embedding_size=100, entities_size=len(ent), min_occurrences=0,\n",
    "                            learning_rate=0.5, batch_size=1000)\n",
    "model.fit_to_corpus(cooccurrence_matrix)\n",
    "#model._GloVeModel__prepare_batches()\n",
    "\n",
    "model.train(num_epochs=3, log_dir=\"log/example\", summary_batch_interval=100)\n",
    "\n",
    "\n",
    "\n",
    "glove_fullspace=model.embeddings#_en\n",
    "\n",
    "print(np.array(glove_fullspace).shape)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "orderd_features_directions=model.embeddings_en\n",
    "\n",
    "write2dArray(glove_fullspace,base_folder_results+'GloveBaseline')\n",
    "write2dArray(orderd_features_directions,base_folder_results+'GloveBaselineOrderd_features_directions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Expert: 0\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 82155.48749278113\n",
      "epoch: 2\n",
      "self.__total_loss 25761.27695196867\n",
      "epoch: 3\n",
      "self.__total_loss 25363.133052092046\n",
      "Starting Expert: 1\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 80596.40917255357\n",
      "epoch: 2\n",
      "self.__total_loss 25687.613186154515\n",
      "epoch: 3\n",
      "self.__total_loss 25406.06386590749\n",
      "Starting Expert: 2\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 81530.28898850456\n",
      "epoch: 2\n",
      "self.__total_loss 25826.06144786626\n",
      "epoch: 3\n",
      "self.__total_loss 25463.156945176423\n",
      "Starting Expert: 3\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 81848.67361267284\n",
      "epoch: 2\n",
      "self.__total_loss 25723.310946118087\n",
      "epoch: 3\n",
      "self.__total_loss 25395.551794193685\n",
      "Starting Expert: 4\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 81884.58081559092\n",
      "epoch: 2\n",
      "self.__total_loss 25792.04779295996\n",
      "epoch: 3\n",
      "self.__total_loss 25436.984502669424\n",
      "Starting Expert: 5\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 81315.83279640228\n",
      "epoch: 2\n",
      "self.__total_loss 25720.192637614906\n",
      "epoch: 3\n",
      "self.__total_loss 25406.309223361313\n",
      "Starting Expert: 6\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 81802.0730985105\n",
      "epoch: 2\n",
      "self.__total_loss 25768.505729410797\n",
      "epoch: 3\n",
      "self.__total_loss 25411.278534095734\n",
      "Starting Expert: 7\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 80789.35145606846\n",
      "epoch: 2\n",
      "self.__total_loss 25721.644326493144\n",
      "epoch: 3\n",
      "self.__total_loss 25420.790400207043\n",
      "Starting Expert: 8\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 80920.56750235334\n",
      "epoch: 2\n",
      "self.__total_loss 25695.821418099105\n",
      "epoch: 3\n",
      "self.__total_loss 25399.598500557244\n",
      "Starting Expert: 9\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 81105.98485264927\n",
      "epoch: 2\n",
      "self.__total_loss 25710.037827424705\n",
      "epoch: 3\n",
      "self.__total_loss 25432.841371431947\n",
      "(10, 3721)\n",
      "(10, 9368)\n",
      "(10, 3721, 10)\n",
      "(10, 9368, 10)\n",
      "(10, 9368)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[263]:\n",
    "#training the experts using the standard glove at the beginning and using the resulting space as \n",
    "#input to our model instead of using  the random initialization\n",
    "\n",
    "models=[]\n",
    "experts=10\n",
    "for i in range(experts):\n",
    "    models.append(GloVeModel(cooccurrence_matrix,fea,ent,embedding_size=10, entities_size=len(ent), min_occurrences=0,\n",
    "                            learning_rate=0.05, batch_size=1000))\n",
    "empEnt=[]\n",
    "empFea=[]\n",
    "errFea=[]\n",
    "entbias=[]\n",
    "feabias=[]\n",
    "for j in range(experts):\n",
    "    print('Starting Expert:', j)\n",
    "    models[j].fit_to_corpus(cooccurrence_matrix)\n",
    "    models[j].train(num_epochs=3, log_dir=\"log/example\", summary_batch_interval=100)\n",
    "    feabias.append(models[j].context_biasesE)\n",
    "    entbias.append(models[j].focal_biasesW)\n",
    "    empFea.append(models[j].embeddings_en)\n",
    "    empEnt.append(models[j].embeddings)\n",
    "    errFea.append(models[j].errorEstimation)\n",
    "\n",
    "\n",
    "# In[264]:\n",
    "\n",
    "\n",
    "print(np.array(entbias).shape)\n",
    "print(np.array(feabias).shape)\n",
    "print(np.array(empEnt).shape)\n",
    "print(np.array(empFea).shape)\n",
    "print(np.array(errFea).shape)\n",
    "#print(errFea)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9368, 10)\n"
     ]
    }
   ],
   "source": [
    "# This function to calculate the error estimation which will be used  as input to the gating network\n",
    "def scoreS(error_Estimation):\n",
    "    S=[]\n",
    "    for i in range(len(fea)):\n",
    "        t=[]\n",
    "        temp=[]\n",
    "        stemp=[]\n",
    "        error_Estimation=np.array(error_Estimation)\n",
    "        temp1=np.array(error_Estimation)[:,i]\n",
    "        temp1=temp1-temp1[np.argmin(temp1)]\n",
    "        \n",
    "        for k in range(experts):\n",
    "            temp.append(np.exp(-temp1[k]))\n",
    "            #print(temp)\n",
    "        \n",
    "        for k in range(experts):\n",
    "            stemp.append(temp[k]/(np.sum(np.array(temp))))\n",
    "        \n",
    "        \n",
    "        #t.append(stemp)\n",
    "        S.append(stemp)\n",
    "    return S\n",
    "    \n",
    "\n",
    "def E_step(errFea):\n",
    "    \n",
    "    scores=scoreS(np.array(errFea))\n",
    "    return scores#,error_Estimation\n",
    "scores=E_step(errFea)\n",
    "print(np.array(scores).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moeGate(scores,gloveVectors):\n",
    "    learning_rate=0.99\n",
    "    n_x=300\n",
    "    n_y=10\n",
    "    lossList=[]\n",
    "    g_tb = tf.Graph()\n",
    "    #\n",
    "    with g_tb.as_default():\n",
    "        x = tf.placeholder(tf.float32, [None, n_x], name=\"x\")#Glove Vectors\n",
    "        y = tf.placeholder(tf.float32, [None, n_y], name=\"y\")#scores\n",
    "        with tf.name_scope('Neural_Nt'):\n",
    "            fully_connected1 = tf.contrib.layers.fully_connected(inputs=x, num_outputs=500,\n",
    "                                                             activation_fn=tf.nn.relu,scope=\"Fully_Conn1\")\n",
    "            fully_connected2 = tf.contrib.layers.fully_connected(inputs=fully_connected1, num_outputs=400, \n",
    "                                                                 activation_fn=tf.nn.relu,scope=\"Fully_Conn2\")\n",
    "            \n",
    "            prediction = tf.contrib.layers.fully_connected(inputs=fully_connected1, num_outputs=10, \n",
    "                                                   activation_fn=tf.nn.softmax,scope=\"Out\")\n",
    "        with tf.name_scope('Cost'):\n",
    "            cost = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=prediction,scope=\"Cost_Function\")\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(prediction, 1, name=\"Argmax_Pred\"), tf.argmax(y, 1, name=\"Y_Pred\"), \n",
    "                                          name=\"Correct_Pred\")\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32, name=\"Cast_Corr_Pred\"), name=\"Accuracy\")\n",
    "     #       \n",
    "        with tf.name_scope('Optimization'):\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate, name=\"Optimizer\").minimize(cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "      #  \n",
    "        # Define variables to be saved in summary (and then displayed with TensorBoard)\n",
    "        \n",
    "    # Start session\n",
    "    with tf.Session(graph=g_tb) as sess:\n",
    "    #\n",
    "        lossList=[]\n",
    "\n",
    "        sess.run(init)\n",
    "        #Save the graph in the summary\n",
    "        #summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        training_epochs = 100\n",
    "        batchSize=1171\n",
    "        for epoch in range(training_epochs):\n",
    "            c1=0\n",
    "            pos = 0\n",
    "            predictionFinal=[]\n",
    "            while pos < len(scores):\n",
    "                _, c ,prediction1,acc= sess.run([optimizer, cost,prediction,accuracy], \n",
    "                                                feed_dict={x: np.array(gloveVectors)[pos:pos+batchSize],\n",
    "                                                         y: np.array(scores)[pos:pos+batchSize]})\n",
    "                for pred in prediction1:\n",
    "                    predictionFinal.append(pred)\n",
    "                pos=pos+batchSize\n",
    "                c1=c1+c\n",
    "            # For every epoch save cost and accuracy\n",
    "            #print(epoch, c1)\n",
    "            lossList.append(c1)\n",
    "        print('Training the gating network is finished')\n",
    "    return predictionFinal,lossList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the gating network is finished\n"
     ]
    }
   ],
   "source": [
    "g_k_J,lossList=moeGate(np.array(scores),gloveVectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM algorithm is used to train the experts and the gating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------ROUND: 0 --------------------------\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "Starting Expert: 0\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 28377.68284655735\n",
      "epoch: 2\n",
      "self.__total_loss 22988.497075997293\n",
      "epoch: 3\n",
      "self.__total_loss 21448.648297287524\n",
      "epoch: 4\n",
      "self.__total_loss 20801.25652964413\n",
      "epoch: 5\n",
      "self.__total_loss 20378.308738892898\n",
      "epoch: 6\n",
      "self.__total_loss 20113.273262469098\n",
      "epoch: 7\n",
      "self.__total_loss 19927.157527130097\n",
      "epoch: 8\n",
      "self.__total_loss 19788.631108848378\n",
      "epoch: 9\n",
      "self.__total_loss 19659.817128561437\n",
      "epoch: 10\n",
      "self.__total_loss 19535.31298253499\n",
      "Starting Expert: 1\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 18646.52318285033\n",
      "epoch: 2\n",
      "self.__total_loss 15986.039944820106\n",
      "epoch: 3\n",
      "self.__total_loss 15194.720253240317\n",
      "epoch: 4\n",
      "self.__total_loss 14662.548062374815\n",
      "epoch: 5\n",
      "self.__total_loss 14451.229641731828\n",
      "epoch: 6\n",
      "self.__total_loss 14319.475272350013\n",
      "epoch: 7\n",
      "self.__total_loss 14206.703908989206\n",
      "epoch: 8\n",
      "self.__total_loss 14115.919231321663\n",
      "epoch: 9\n",
      "self.__total_loss 14041.573382206261\n",
      "epoch: 10\n",
      "self.__total_loss 13983.25802028738\n",
      "Starting Expert: 2\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 18572.733251642436\n",
      "epoch: 2\n",
      "self.__total_loss 15954.182676026598\n",
      "epoch: 3\n",
      "self.__total_loss 15185.444565201178\n",
      "epoch: 4\n",
      "self.__total_loss 14586.529186936095\n",
      "epoch: 5\n",
      "self.__total_loss 14404.253703121096\n",
      "epoch: 6\n",
      "self.__total_loss 14230.572560757399\n",
      "epoch: 7\n",
      "self.__total_loss 14112.707479277626\n",
      "epoch: 8\n",
      "self.__total_loss 14033.155749611557\n",
      "epoch: 9\n",
      "self.__total_loss 13978.808794034645\n",
      "epoch: 10\n",
      "self.__total_loss 13918.212008416653\n",
      "Starting Expert: 3\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 22648.091373149306\n",
      "epoch: 2\n",
      "self.__total_loss 19258.947895519435\n",
      "epoch: 3\n",
      "self.__total_loss 18192.60156618245\n",
      "epoch: 4\n",
      "self.__total_loss 17741.269431669265\n",
      "epoch: 5\n",
      "self.__total_loss 17532.109154148027\n",
      "epoch: 6\n",
      "self.__total_loss 17369.996474388987\n",
      "epoch: 7\n",
      "self.__total_loss 17223.842933822423\n",
      "epoch: 8\n",
      "self.__total_loss 17122.52174991928\n",
      "epoch: 9\n",
      "self.__total_loss 17033.591464309022\n",
      "epoch: 10\n",
      "self.__total_loss 16958.375862991437\n",
      "Starting Expert: 4\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 22238.4467516914\n",
      "epoch: 2\n",
      "self.__total_loss 18954.308848686516\n",
      "epoch: 3\n",
      "self.__total_loss 17691.720277991146\n",
      "epoch: 4\n",
      "self.__total_loss 17205.275273356587\n",
      "epoch: 5\n",
      "self.__total_loss 16960.556664261967\n",
      "epoch: 6\n",
      "self.__total_loss 16737.694395851344\n",
      "epoch: 7\n",
      "self.__total_loss 16579.7510639932\n",
      "epoch: 8\n",
      "self.__total_loss 16454.42784736678\n",
      "epoch: 9\n",
      "self.__total_loss 16364.92489460297\n",
      "epoch: 10\n",
      "self.__total_loss 16290.229691686109\n",
      "Starting Expert: 5\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 21559.378847721964\n",
      "epoch: 2\n",
      "self.__total_loss 18369.51496564597\n",
      "epoch: 3\n",
      "self.__total_loss 17430.924811035395\n",
      "epoch: 4\n",
      "self.__total_loss 16793.358807345852\n",
      "epoch: 5\n",
      "self.__total_loss 16457.598961977288\n",
      "epoch: 6\n",
      "self.__total_loss 16322.975497812033\n",
      "epoch: 7\n",
      "self.__total_loss 16149.050381762907\n",
      "epoch: 8\n",
      "self.__total_loss 16026.547240206972\n",
      "epoch: 9\n",
      "self.__total_loss 15938.84833830595\n",
      "epoch: 10\n",
      "self.__total_loss 15865.070878900588\n",
      "Starting Expert: 6\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 25788.990023937076\n",
      "epoch: 2\n",
      "self.__total_loss 21494.207138612866\n",
      "epoch: 3\n",
      "self.__total_loss 20414.330345503986\n",
      "epoch: 4\n",
      "self.__total_loss 19832.338879477233\n",
      "epoch: 5\n",
      "self.__total_loss 19451.914649523795\n",
      "epoch: 6\n",
      "self.__total_loss 19204.17976996675\n",
      "epoch: 7\n",
      "self.__total_loss 19015.336987700313\n",
      "epoch: 8\n",
      "self.__total_loss 18887.63067488931\n",
      "epoch: 9\n",
      "self.__total_loss 18764.498849833384\n",
      "epoch: 10\n",
      "self.__total_loss 18661.27934527211\n",
      "Starting Expert: 7\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 19227.702098000795\n",
      "epoch: 2\n",
      "self.__total_loss 16507.778637364507\n",
      "epoch: 3\n",
      "self.__total_loss 15558.483385723084\n",
      "epoch: 4\n",
      "self.__total_loss 15017.52470709756\n",
      "epoch: 5\n",
      "self.__total_loss 14815.941891739145\n",
      "epoch: 6\n",
      "self.__total_loss 14686.776583667845\n",
      "epoch: 7\n",
      "self.__total_loss 14563.446807472035\n",
      "epoch: 8\n",
      "self.__total_loss 14466.479935524985\n",
      "epoch: 9\n",
      "self.__total_loss 14394.574848735705\n",
      "epoch: 10\n",
      "self.__total_loss 14336.390653308481\n",
      "Starting Expert: 8\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 22032.42378122732\n",
      "epoch: 2\n",
      "self.__total_loss 18754.24631016329\n",
      "epoch: 3\n",
      "self.__total_loss 17807.488268958405\n",
      "epoch: 4\n",
      "self.__total_loss 17315.88176592253\n",
      "epoch: 5\n",
      "self.__total_loss 16995.156056890264\n",
      "epoch: 6\n",
      "self.__total_loss 16777.869915122166\n",
      "epoch: 7\n",
      "self.__total_loss 16637.94526147656\n",
      "epoch: 8\n",
      "self.__total_loss 16524.972338039428\n",
      "epoch: 9\n",
      "self.__total_loss 16433.684542756528\n",
      "epoch: 10\n",
      "self.__total_loss 16364.095494884998\n",
      "Starting Expert: 9\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 20294.088821165264\n",
      "epoch: 2\n",
      "self.__total_loss 16818.236361548305\n",
      "epoch: 3\n",
      "self.__total_loss 15609.222348157316\n",
      "epoch: 4\n",
      "self.__total_loss 15031.0820553042\n",
      "epoch: 5\n",
      "self.__total_loss 14834.099645068869\n",
      "epoch: 6\n",
      "self.__total_loss 14744.52077639848\n",
      "epoch: 7\n",
      "self.__total_loss 14574.652677468956\n",
      "epoch: 8\n",
      "self.__total_loss 14450.240390704945\n",
      "epoch: 9\n",
      "self.__total_loss 14366.889933383092\n",
      "epoch: 10\n",
      "self.__total_loss 14318.72982130386\n",
      "(9368, 10)\n",
      "Training the gating network is finished\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------ROUND: 1 --------------------------\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "Starting Expert: 0\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 17631.87365821749\n",
      "epoch: 2\n",
      "self.__total_loss 14753.931812856346\n",
      "epoch: 3\n",
      "self.__total_loss 13112.614608662203\n",
      "epoch: 4\n",
      "self.__total_loss 12598.672111667693\n",
      "epoch: 5\n",
      "self.__total_loss 12455.741877894849\n",
      "epoch: 6\n",
      "self.__total_loss 12340.254233086482\n",
      "epoch: 7\n",
      "self.__total_loss 12228.676405934617\n",
      "epoch: 8\n",
      "self.__total_loss 12070.760412825271\n",
      "epoch: 9\n",
      "self.__total_loss 11935.301519571804\n",
      "epoch: 10\n",
      "self.__total_loss 11830.856407506391\n",
      "Starting Expert: 1\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 17034.878801092505\n",
      "epoch: 2\n",
      "self.__total_loss 15511.280121669173\n",
      "epoch: 3\n",
      "self.__total_loss 14986.619253225625\n",
      "epoch: 4\n",
      "self.__total_loss 14742.467107659206\n",
      "epoch: 5\n",
      "self.__total_loss 14555.610475620255\n",
      "epoch: 6\n",
      "self.__total_loss 14422.172115277499\n",
      "epoch: 7\n",
      "self.__total_loss 14317.268953964114\n",
      "epoch: 8\n",
      "self.__total_loss 14234.078900007531\n",
      "epoch: 9\n",
      "self.__total_loss 14169.64583539404\n",
      "epoch: 10\n",
      "self.__total_loss 14108.389986937866\n",
      "Starting Expert: 2\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 15990.77908713743\n",
      "epoch: 2\n",
      "self.__total_loss 14690.766492765397\n",
      "epoch: 3\n",
      "self.__total_loss 13969.999012701213\n",
      "epoch: 4\n",
      "self.__total_loss 13758.739515956491\n",
      "epoch: 5\n",
      "self.__total_loss 13655.347722057253\n",
      "epoch: 6\n",
      "self.__total_loss 13528.480870585889\n",
      "epoch: 7\n",
      "self.__total_loss 13407.249730592594\n",
      "epoch: 8\n",
      "self.__total_loss 13319.990213878453\n",
      "epoch: 9\n",
      "self.__total_loss 13260.839034574106\n",
      "epoch: 10\n",
      "self.__total_loss 13208.573361147195\n",
      "Starting Expert: 3\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 31053.12713548541\n",
      "epoch: 2\n",
      "self.__total_loss 26493.63254774362\n",
      "epoch: 3\n",
      "self.__total_loss 25288.638401877135\n",
      "epoch: 4\n",
      "self.__total_loss 24591.500001769513\n",
      "epoch: 5\n",
      "self.__total_loss 24164.86756889522\n",
      "epoch: 6\n",
      "self.__total_loss 23867.96281803772\n",
      "epoch: 7\n",
      "self.__total_loss 23657.241717699915\n",
      "epoch: 8\n",
      "self.__total_loss 23458.192830875516\n",
      "epoch: 9\n",
      "self.__total_loss 23297.490520175546\n",
      "epoch: 10\n",
      "self.__total_loss 23174.69113267958\n",
      "Starting Expert: 4\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 17991.29980725795\n",
      "epoch: 2\n",
      "self.__total_loss 15687.306284286082\n",
      "epoch: 3\n",
      "self.__total_loss 14788.63748360239\n",
      "epoch: 4\n",
      "self.__total_loss 14523.530939830467\n",
      "epoch: 5\n",
      "self.__total_loss 14455.525986000896\n",
      "epoch: 6\n",
      "self.__total_loss 14313.856974706054\n",
      "epoch: 7\n",
      "self.__total_loss 14156.360416913405\n",
      "epoch: 8\n",
      "self.__total_loss 14055.415935108438\n",
      "epoch: 9\n",
      "self.__total_loss 13982.764641055837\n",
      "epoch: 10\n",
      "self.__total_loss 13933.196113146842\n",
      "Starting Expert: 5\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 13384.029291376472\n",
      "epoch: 2\n",
      "self.__total_loss 12468.674323478714\n",
      "epoch: 3\n",
      "self.__total_loss 11889.449086140841\n",
      "epoch: 4\n",
      "self.__total_loss 11676.58991237916\n",
      "epoch: 5\n",
      "self.__total_loss 11566.397254893556\n",
      "epoch: 6\n",
      "self.__total_loss 11464.898302123882\n",
      "epoch: 7\n",
      "self.__total_loss 11382.652415279299\n",
      "epoch: 8\n",
      "self.__total_loss 11325.009230908938\n",
      "epoch: 9\n",
      "self.__total_loss 11278.39390087314\n",
      "epoch: 10\n",
      "self.__total_loss 11236.025749498978\n",
      "Starting Expert: 6\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 11143.648793492466\n",
      "epoch: 2\n",
      "self.__total_loss 10391.740984614938\n",
      "epoch: 3\n",
      "self.__total_loss 10123.823591241613\n",
      "epoch: 4\n",
      "self.__total_loss 9979.444162874483\n",
      "epoch: 5\n",
      "self.__total_loss 9872.917057347484\n",
      "epoch: 6\n",
      "self.__total_loss 9796.811088846996\n",
      "epoch: 7\n",
      "self.__total_loss 9740.879669140093\n",
      "epoch: 8\n",
      "self.__total_loss 9693.291417332366\n",
      "epoch: 9\n",
      "self.__total_loss 9654.307685600594\n",
      "epoch: 10\n",
      "self.__total_loss 9619.391922132112\n",
      "Starting Expert: 7\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 31212.00776388496\n",
      "epoch: 2\n",
      "self.__total_loss 27071.76777485013\n",
      "epoch: 3\n",
      "self.__total_loss 25467.10198442638\n",
      "epoch: 4\n",
      "self.__total_loss 24620.0222264342\n",
      "epoch: 5\n",
      "self.__total_loss 24131.14064507559\n",
      "epoch: 6\n",
      "self.__total_loss 23817.291190493852\n",
      "epoch: 7\n",
      "self.__total_loss 23566.90448236838\n",
      "epoch: 8\n",
      "self.__total_loss 23414.52750954032\n",
      "epoch: 9\n",
      "self.__total_loss 23232.863263593987\n",
      "epoch: 10\n",
      "self.__total_loss 23096.810204051435\n",
      "Starting Expert: 8\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 16540.274730879813\n",
      "epoch: 2\n",
      "self.__total_loss 14836.703638508916\n",
      "epoch: 3\n",
      "self.__total_loss 14260.593887092546\n",
      "epoch: 4\n",
      "self.__total_loss 14014.68836376071\n",
      "epoch: 5\n",
      "self.__total_loss 13851.298725035042\n",
      "epoch: 6\n",
      "self.__total_loss 13702.854482928291\n",
      "epoch: 7\n",
      "self.__total_loss 13608.012477921322\n",
      "epoch: 8\n",
      "self.__total_loss 13515.623737623915\n",
      "epoch: 9\n",
      "self.__total_loss 13457.570708885789\n",
      "epoch: 10\n",
      "self.__total_loss 13387.57471298799\n",
      "Starting Expert: 9\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 32147.369902454317\n",
      "epoch: 2\n",
      "self.__total_loss 25935.888108171523\n",
      "epoch: 3\n",
      "self.__total_loss 23186.065298333764\n",
      "epoch: 4\n",
      "self.__total_loss 22248.844926334918\n",
      "epoch: 5\n",
      "self.__total_loss 21778.238966871053\n",
      "epoch: 6\n",
      "self.__total_loss 21513.28570232913\n",
      "epoch: 7\n",
      "self.__total_loss 21291.46032020822\n",
      "epoch: 8\n",
      "self.__total_loss 21105.35845516622\n",
      "epoch: 9\n",
      "self.__total_loss 20972.575277663767\n",
      "epoch: 10\n",
      "self.__total_loss 20851.082928150892\n",
      "(9368, 10)\n",
      "Training the gating network is finished\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nssAkUxPWFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------ROUND: 2 --------------------------\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "len(entities) 9368\n",
      "Starting Expert: 0\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 17254.246800217777\n",
      "epoch: 2\n",
      "self.__total_loss 14192.461486550048\n",
      "epoch: 3\n",
      "self.__total_loss 12601.362223763019\n",
      "epoch: 4\n",
      "self.__total_loss 12102.071052778512\n",
      "epoch: 5\n",
      "self.__total_loss 11969.387277217582\n",
      "epoch: 6\n",
      "self.__total_loss 11931.29218922928\n",
      "epoch: 7\n",
      "self.__total_loss 11768.575367681682\n",
      "epoch: 8\n",
      "self.__total_loss 11584.940194536\n",
      "epoch: 9\n",
      "self.__total_loss 11459.925986701623\n",
      "epoch: 10\n",
      "self.__total_loss 11373.383864066564\n",
      "Starting Expert: 1\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 22602.860929004848\n",
      "epoch: 2\n",
      "self.__total_loss 19940.00038640946\n",
      "epoch: 3\n",
      "self.__total_loss 19084.026728060097\n",
      "epoch: 4\n",
      "self.__total_loss 18576.776973018423\n",
      "epoch: 5\n",
      "self.__total_loss 18293.7362344563\n",
      "epoch: 6\n",
      "self.__total_loss 18095.65416248329\n",
      "epoch: 7\n",
      "self.__total_loss 17904.445349998772\n",
      "epoch: 8\n",
      "self.__total_loss 17773.656691977754\n",
      "epoch: 9\n",
      "self.__total_loss 17665.602420385927\n",
      "epoch: 10\n",
      "self.__total_loss 17573.377759948373\n",
      "Starting Expert: 2\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 20375.314514461905\n",
      "epoch: 2\n",
      "self.__total_loss 17617.997514570132\n",
      "epoch: 3\n",
      "self.__total_loss 16760.14593030885\n",
      "epoch: 4\n",
      "self.__total_loss 16347.582768486813\n",
      "epoch: 5\n",
      "self.__total_loss 16035.421681672335\n",
      "epoch: 6\n",
      "self.__total_loss 15814.855434019119\n",
      "epoch: 7\n",
      "self.__total_loss 15629.033753968775\n",
      "epoch: 8\n",
      "self.__total_loss 15495.168915981427\n",
      "epoch: 9\n",
      "self.__total_loss 15377.524579329416\n",
      "epoch: 10\n",
      "self.__total_loss 15274.046461196616\n",
      "Starting Expert: 3\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 31706.64091104269\n",
      "epoch: 2\n",
      "self.__total_loss 24958.40060498938\n",
      "epoch: 3\n",
      "self.__total_loss 23035.583328679204\n",
      "epoch: 4\n",
      "self.__total_loss 22170.207800781354\n",
      "epoch: 5\n",
      "self.__total_loss 21738.953741837293\n",
      "epoch: 6\n",
      "self.__total_loss 21430.66530074738\n",
      "epoch: 7\n",
      "self.__total_loss 21176.224792106077\n",
      "epoch: 8\n",
      "self.__total_loss 20968.251905377954\n",
      "epoch: 9\n",
      "self.__total_loss 20787.813640130684\n",
      "epoch: 10\n",
      "self.__total_loss 20628.66314872168\n",
      "Starting Expert: 4\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 18144.927432034165\n",
      "epoch: 2\n",
      "self.__total_loss 15481.669174082577\n",
      "epoch: 3\n",
      "self.__total_loss 14513.25428918004\n",
      "epoch: 4\n",
      "self.__total_loss 14180.42476507835\n",
      "epoch: 5\n",
      "self.__total_loss 14044.811150707304\n",
      "epoch: 6\n",
      "self.__total_loss 13897.809924775735\n",
      "epoch: 7\n",
      "self.__total_loss 13718.844077520072\n",
      "epoch: 8\n",
      "self.__total_loss 13589.495158601552\n",
      "epoch: 9\n",
      "self.__total_loss 13491.59614686668\n",
      "epoch: 10\n",
      "self.__total_loss 13435.28423718363\n",
      "Starting Expert: 5\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 20021.719749234617\n",
      "epoch: 2\n",
      "self.__total_loss 17680.957318246365\n",
      "epoch: 3\n",
      "self.__total_loss 17144.313961938024\n",
      "epoch: 4\n",
      "self.__total_loss 16748.880196411163\n",
      "epoch: 5\n",
      "self.__total_loss 16538.627218339592\n",
      "epoch: 6\n",
      "self.__total_loss 16378.768602654338\n",
      "epoch: 7\n",
      "self.__total_loss 16255.697540098801\n",
      "epoch: 8\n",
      "self.__total_loss 16146.036369666457\n",
      "epoch: 9\n",
      "self.__total_loss 16027.420908825472\n",
      "epoch: 10\n",
      "self.__total_loss 15939.96532405354\n",
      "Starting Expert: 6\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 12760.377090673894\n",
      "epoch: 2\n",
      "self.__total_loss 11760.723522199318\n",
      "epoch: 3\n",
      "self.__total_loss 11383.284159298986\n",
      "epoch: 4\n",
      "self.__total_loss 11212.48286736384\n",
      "epoch: 5\n",
      "self.__total_loss 11084.006396094337\n",
      "epoch: 6\n",
      "self.__total_loss 10971.517263308167\n",
      "epoch: 7\n",
      "self.__total_loss 10899.36872446537\n",
      "epoch: 8\n",
      "self.__total_loss 10842.854437059723\n",
      "epoch: 9\n",
      "self.__total_loss 10796.409989390522\n",
      "epoch: 10\n",
      "self.__total_loss 10753.566604443826\n",
      "Starting Expert: 7\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 23618.18701571226\n",
      "epoch: 2\n",
      "self.__total_loss 18272.954591114074\n",
      "epoch: 3\n",
      "self.__total_loss 16591.898263130337\n",
      "epoch: 4\n",
      "self.__total_loss 16014.107519695535\n",
      "epoch: 5\n",
      "self.__total_loss 15772.951301788911\n",
      "epoch: 6\n",
      "self.__total_loss 15590.21499353461\n",
      "epoch: 7\n",
      "self.__total_loss 15332.88387689367\n",
      "epoch: 8\n",
      "self.__total_loss 15105.606844961643\n",
      "epoch: 9\n",
      "self.__total_loss 14939.473239826038\n",
      "epoch: 10\n",
      "self.__total_loss 14823.311954796314\n",
      "Starting Expert: 8\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 20401.268692709506\n",
      "epoch: 2\n",
      "self.__total_loss 17458.408078815788\n",
      "epoch: 3\n",
      "self.__total_loss 15909.9990577735\n",
      "epoch: 4\n",
      "self.__total_loss 15479.366194281727\n",
      "epoch: 5\n",
      "self.__total_loss 15262.397105263546\n",
      "epoch: 6\n",
      "self.__total_loss 15033.901359185576\n",
      "epoch: 7\n",
      "self.__total_loss 14826.63813816756\n",
      "epoch: 8\n",
      "self.__total_loss 14664.99255762063\n",
      "epoch: 9\n",
      "self.__total_loss 14536.848593698815\n",
      "epoch: 10\n",
      "self.__total_loss 14425.894563443959\n",
      "Starting Expert: 9\n",
      "(1000,)\n",
      "single_lose (1000,)\n",
      "Writing TensorBoard summaries to log/example\n",
      "epoch: 1\n",
      "self.__total_loss 21846.110706053674\n",
      "epoch: 2\n",
      "self.__total_loss 18809.635731354356\n",
      "epoch: 3\n",
      "self.__total_loss 15936.480248318985\n",
      "epoch: 4\n",
      "self.__total_loss 15126.233661210164\n",
      "epoch: 5\n",
      "self.__total_loss 14734.733593057841\n",
      "epoch: 6\n",
      "self.__total_loss 14524.946465548128\n",
      "epoch: 7\n",
      "self.__total_loss 14327.429012987763\n",
      "epoch: 8\n",
      "self.__total_loss 14177.097718929872\n",
      "epoch: 9\n",
      "self.__total_loss 14020.246909858659\n",
      "epoch: 10\n",
      "self.__total_loss 13904.450993567705\n",
      "(9368, 10)\n",
      "Training the gating network is finished\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAPi/fL+gRzoG030AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------Here What Iam Looking For--------------------------------------\n",
      "(9368, 10)\n",
      "\n",
      "\n",
      "----------------Expert: 0 ---------------------\n",
      "students 0.9702545\n",
      "college 0.96757466\n",
      "student 0.9661591\n",
      "school 0.93698925\n",
      "campus 0.93564266\n",
      "faculty 0.9134759\n",
      "semester 0.9058842\n",
      "colleges 0.8937213\n",
      "programs 0.8927386\n",
      "byu 0.8902891\n",
      "undergraduate 0.8894873\n",
      "university 0.8820826\n",
      "program 0.87141293\n",
      "schools 0.86461663\n",
      "graduate 0.8634738\n",
      "academic 0.8457893\n",
      "campuses 0.839512\n",
      "graduation 0.8255658\n",
      "education 0.8165045\n",
      "tuition 0.7948647\n",
      "freshmen 0.7904045\n",
      "enrollment 0.773103\n",
      "graduates 0.7476339\n",
      "classroom 0.73600537\n",
      "degree 0.7281495\n",
      "teacher 0.71957177\n",
      "teachers 0.7161352\n",
      "undergraduates 0.7031194\n",
      "curriculum 0.69958377\n",
      "scholarships 0.69776714\n",
      "scholarship 0.6793792\n",
      "classrooms 0.6558641\n",
      "year 0.6533017\n",
      "years 0.6507316\n",
      "elementary 0.6466681\n",
      "universities 0.62719035\n",
      "alumni 0.5972328\n",
      "professors 0.5959414\n",
      "teaching 0.593502\n",
      "diploma 0.58340394\n",
      "kindergarten 0.5683379\n",
      "extracurricular 0.56309295\n",
      "admissions 0.55749625\n",
      "auditorium 0.5563243\n",
      "courses 0.55531454\n",
      "doctoral 0.549026\n",
      "dormitory 0.54147166\n",
      "grades 0.52196115\n",
      "classes 0.5074806\n",
      "educational 0.5073747\n",
      "commencement 0.50125337\n",
      "majors 0.49372542\n",
      "halls 0.48443595\n",
      "coeducational 0.48076737\n",
      "room 0.46616125\n",
      "pupils 0.4655537\n",
      "degrees 0.46321112\n",
      "cafeteria 0.46207243\n",
      "dormitories 0.4455176\n",
      "deans 0.44549772\n",
      "bachelor 0.44406754\n",
      "postgraduate 0.4411794\n",
      "postgraduate 0.4411794\n",
      "alumnus 0.44021076\n",
      "institution 0.43826717\n",
      "diplomas 0.43429056\n",
      "arts 0.43280193\n",
      "alma 0.43220055\n",
      "seminary 0.43153468\n",
      "months 0.4300852\n",
      "learning 0.4277778\n",
      "hall 0.42568746\n",
      "postsecondary 0.42179686\n",
      "rotc 0.4191455\n",
      "administrators 0.41783178\n",
      "athletics 0.40795258\n",
      "faculties 0.38895726\n",
      "salle 0.38501516\n",
      "math 0.3847534\n",
      "applicants 0.37918353\n",
      "professor 0.3772563\n",
      "journalism 0.37471083\n",
      "intercollegiate 0.3737506\n",
      "convocation 0.3673561\n",
      "programmes 0.36701307\n",
      "stanford 0.36534742\n",
      "days 0.3642793\n",
      "grade 0.36243537\n",
      "intensive 0.36005855\n",
      "hallway 0.35971242\n",
      "taught 0.35936895\n",
      "varsity 0.35880637\n",
      "admission 0.35066766\n",
      "older 0.34994608\n",
      "psychology 0.34760758\n",
      "cornell 0.347546\n",
      "nursing 0.34737226\n",
      "educators 0.3471311\n",
      "recipients 0.34666488\n",
      "academics 0.34612635\n",
      "\n",
      "\n",
      "----------------Expert: 1 ---------------------\n",
      "britain 0.95209694\n",
      "welsh 0.9509204\n",
      "wales 0.9464354\n",
      "england 0.94480795\n",
      "english 0.89996123\n",
      "british 0.87604713\n",
      "iran 0.87200755\n",
      "ireland 0.8706245\n",
      "castle 0.8404346\n",
      "cardiff 0.83027285\n",
      "scotland 0.8293437\n",
      "engineering 0.82297605\n",
      "swansea 0.8228793\n",
      "europe 0.82019943\n",
      "scottish 0.8170979\n",
      "gothic 0.816715\n",
      "coal 0.7971783\n",
      "technology 0.78430206\n",
      "london 0.77889544\n",
      "germany 0.761548\n",
      "castles 0.75141495\n",
      "industrial 0.74350554\n",
      "medieval 0.7364303\n",
      "davies 0.7328519\n",
      "european 0.7313141\n",
      "assembly 0.72820544\n",
      "bbc 0.7055111\n",
      "irish 0.7017461\n",
      "nuclear 0.6850336\n",
      "windsor 0.68416375\n",
      "french 0.6690373\n",
      "cathedral 0.66099703\n",
      "france 0.6503334\n",
      "italy 0.6437843\n",
      "german 0.63834006\n",
      "spain 0.6378429\n",
      "spanish 0.6335763\n",
      "country 0.6221972\n",
      "liverpool 0.6128079\n",
      "assemblies 0.6063426\n",
      "newcastle 0.59454113\n",
      "physics 0.589822\n",
      "upper 0.58462644\n",
      "robinson 0.581594\n",
      "dublin 0.57876736\n",
      "aerospace 0.57779026\n",
      "countries 0.577042\n",
      "leeds 0.5732863\n",
      "egypt 0.57175124\n",
      "bolton 0.5689025\n",
      "mining 0.56278986\n",
      "oxford 0.56241924\n",
      "edward 0.5616261\n",
      "russia 0.5541178\n",
      "abbey 0.5527305\n",
      "chapel 0.5501885\n",
      "glasgow 0.54209596\n",
      "railways 0.5387787\n",
      "cambridge 0.53636473\n",
      "empire 0.5330118\n",
      "yorkshire 0.53112966\n",
      "roman 0.52963305\n",
      "albanian 0.529555\n",
      "greek 0.52824455\n",
      "revolution 0.5272298\n",
      "mines 0.5271387\n",
      "greece 0.5271143\n",
      "greeks 0.5248226\n",
      "china 0.5226591\n",
      "italian 0.5206867\n",
      "syria 0.5193917\n",
      "midlands 0.5187718\n",
      "manchester 0.5185266\n",
      "parliament 0.5170956\n",
      "labour 0.51635265\n",
      "technologies 0.5146441\n",
      "petroleum 0.511835\n",
      "vauxhall 0.5104987\n",
      "sweden 0.5088878\n",
      "basque 0.50533754\n",
      "grammar 0.50357157\n",
      "ltd 0.5021642\n",
      "gwent 0.5011771\n",
      "industries 0.5000545\n",
      "nations 0.49788147\n",
      "sheffield 0.49589127\n",
      "israel 0.49371296\n",
      "georgian 0.4893163\n",
      "world 0.48898187\n",
      "chemistry 0.48825002\n",
      "celtic 0.48772684\n",
      "monarchy 0.48723364\n",
      "war 0.48642996\n",
      "language 0.48458654\n",
      "kingdom 0.48419982\n",
      "chemical 0.48297215\n",
      "warfare 0.48196867\n",
      "gower 0.4817017\n",
      "plc 0.48099017\n",
      "turbine 0.47907203\n",
      "\n",
      "\n",
      "----------------Expert: 2 ---------------------\n",
      "king 0.9176028\n",
      "hotel 0.9005847\n",
      "queen 0.8668509\n",
      "royal 0.8637559\n",
      "rooms 0.856691\n",
      "hotels 0.8541924\n",
      "palace 0.834505\n",
      "suites 0.67091006\n",
      "kings 0.623475\n",
      "prince 0.6230309\n",
      "accommodations 0.6060337\n",
      "princes 0.590982\n",
      "lodging 0.57793313\n",
      "main 0.5449998\n",
      "sultan 0.5407404\n",
      "large 0.53310895\n",
      "throne 0.5129747\n",
      "emperor 0.50839806\n",
      "palaces 0.50309324\n",
      "queens 0.49661312\n",
      "monarch 0.4917153\n",
      "lodgings 0.48519415\n",
      "resort 0.47114998\n",
      "resorts 0.4660593\n",
      "empress 0.4638267\n",
      "motel 0.4543344\n",
      "hospital 0.45211768\n",
      "amenities 0.44984794\n",
      "regent 0.4498201\n",
      "hostels 0.43805546\n",
      "inns 0.42323372\n",
      "spa 0.4231978\n",
      "suite 0.4206993\n",
      "patients 0.41067997\n",
      "veterinary 0.40947145\n",
      "beds 0.40888223\n",
      "hospitals 0.40857393\n",
      "princess 0.40729147\n",
      "duke 0.40376577\n",
      "spacious 0.40175515\n",
      "ruler 0.39167622\n",
      "accommodation 0.38870898\n",
      "airlines 0.38016912\n",
      "imperial 0.37560838\n",
      "nurse 0.37348297\n",
      "coronation 0.37187245\n",
      "tsar 0.36821902\n",
      "outpatient 0.3671973\n",
      "patient 0.3643295\n",
      "pharmaceutical 0.36296347\n",
      "buffet 0.36258668\n",
      "luxurious 0.3619304\n",
      "heir 0.3559787\n",
      "mattress 0.35223654\n",
      "palais 0.35166875\n",
      "twin 0.3503829\n",
      "nobles 0.35004595\n",
      "lion 0.3499883\n",
      "chamberlain 0.34905186\n",
      "physicians 0.34854636\n",
      "gran 0.34792563\n",
      "larger 0.34527665\n",
      "size 0.3437924\n",
      "reservation 0.3415031\n",
      "flights 0.34145838\n",
      "surgical 0.33954778\n",
      "rulers 0.33843037\n",
      "hereditary 0.33783454\n",
      "physician 0.33512968\n",
      "breakfast 0.33503896\n",
      "consort 0.33443877\n",
      "guests 0.33401588\n",
      "nurses 0.33388373\n",
      "specialty 0.33162895\n",
      "specialties 0.33103183\n",
      "maharaja 0.33092394\n",
      "viceroy 0.33083606\n",
      "smaller 0.33058637\n",
      "royals 0.33026236\n",
      "medical 0.3299639\n",
      "monarchs 0.32202592\n",
      "attendants 0.32082582\n",
      "restaurants 0.31887913\n",
      "guest 0.31658578\n",
      "emperors 0.3157679\n",
      "hostel 0.3154698\n",
      "raja 0.3138602\n",
      "giant 0.3129339\n",
      "dukes 0.3102258\n",
      "clinical 0.3100713\n",
      "enormous 0.30728394\n",
      "scarlet 0.30545476\n",
      "pharmacy 0.3051235\n",
      "earls 0.30510908\n",
      "regency 0.30384263\n",
      "villas 0.3037625\n",
      "diplomatic 0.30278444\n",
      "specialized 0.30048952\n",
      "allied 0.29957482\n",
      "restaurant 0.29869547\n",
      "\n",
      "\n",
      "----------------Expert: 3 ---------------------\n",
      "architect 0.9742193\n",
      "los 0.9674695\n",
      "club 0.95419425\n",
      "game 0.95242435\n",
      "estate 0.95065075\n",
      "basketball 0.95006573\n",
      "george 0.95004\n",
      "party 0.944098\n",
      "mansion 0.94196486\n",
      "california 0.9403998\n",
      "san 0.9397948\n",
      "angeles 0.9368516\n",
      "games 0.9366488\n",
      "men 0.93652\n",
      "member 0.9359669\n",
      "members 0.931987\n",
      "politician 0.92294466\n",
      "john 0.9197545\n",
      "william 0.91647184\n",
      "house 0.91524494\n",
      "design 0.9115973\n",
      "golf 0.90584064\n",
      "governor 0.8992093\n",
      "sir 0.8963273\n",
      "farm 0.8907569\n",
      "property 0.887635\n",
      "james 0.8848525\n",
      "david 0.8838568\n",
      "ranch 0.8790896\n",
      "paul 0.8727918\n",
      "home 0.86764914\n",
      "family 0.8643122\n",
      "adobe 0.8607576\n",
      "leader 0.8582425\n",
      "charles 0.854353\n",
      "lord 0.85417956\n",
      "thomas 0.85385597\n",
      "francisco 0.84694266\n",
      "team 0.84629446\n",
      "businessman 0.83446443\n",
      "construction 0.8306883\n",
      "architecture 0.8287048\n",
      "minister 0.80808085\n",
      "membership 0.7986242\n",
      "robert 0.7958133\n",
      "arena 0.78682667\n",
      "liberal 0.76942426\n",
      "homes 0.7619744\n",
      "richard 0.7539827\n",
      "hollywood 0.746163\n",
      "estates 0.74244213\n",
      "builder 0.739862\n",
      "clubhouse 0.73595625\n",
      "women 0.73338217\n",
      "nba 0.73269916\n",
      "statesman 0.7124027\n",
      "baron 0.7114525\n",
      "polo 0.704363\n",
      "diego 0.7022661\n",
      "football 0.7016622\n",
      "jose 0.70131564\n",
      "presidential 0.69679856\n",
      "residence 0.69212395\n",
      "baseball 0.690655\n",
      "league 0.68370295\n",
      "clubs 0.68095165\n",
      "gay 0.6808737\n",
      "lloyd 0.67352635\n",
      "architects 0.6697041\n",
      "conservative 0.6577583\n",
      "washington 0.6407406\n",
      "gang 0.6314656\n",
      "senator 0.62337303\n",
      "plantation 0.61920893\n",
      "joseph 0.6164326\n",
      "properties 0.6163079\n",
      "designs 0.6120337\n",
      "vegas 0.61110854\n",
      "gentleman 0.608979\n",
      "francis 0.6005535\n",
      "wealthy 0.59160155\n",
      "andrew 0.5877687\n",
      "gentlemen 0.5870421\n",
      "arizona 0.58455217\n",
      "architectural 0.58351\n",
      "votes 0.58186024\n",
      "frank 0.58007246\n",
      "smith 0.5800503\n",
      "season 0.5711176\n",
      "villa 0.5696761\n",
      "manor 0.56843644\n",
      "presidency 0.5637664\n",
      "settlers 0.5633908\n",
      "chancellor 0.55608994\n",
      "rancho 0.5530277\n",
      "barbara 0.5504224\n",
      "parties 0.5458475\n",
      "harry 0.54563403\n",
      "clan 0.5445728\n",
      "turf 0.5392571\n",
      "\n",
      "\n",
      "----------------Expert: 4 ---------------------\n",
      "september 0.9830958\n",
      "november 0.980721\n",
      "december 0.9776694\n",
      "april 0.97302896\n",
      "february 0.9708803\n",
      "october 0.9693439\n",
      "january 0.9670524\n",
      "july 0.96211624\n",
      "august 0.96044576\n",
      "june 0.9519285\n",
      "march 0.90498126\n",
      "sept 0.804663\n",
      "retrieved 0.8039506\n",
      "oct 0.7836598\n",
      "rear 0.70890355\n",
      "side 0.6732649\n",
      "front 0.66474336\n",
      "street 0.65758175\n",
      "newspaper 0.6272114\n",
      "augustus 0.6242837\n",
      "original 0.5989458\n",
      "corner 0.5908219\n",
      "accessed 0.57756233\n",
      "mayo 0.5736956\n",
      "feb 0.554286\n",
      "thursday 0.51991147\n",
      "tuesday 0.5001196\n",
      "saturday 0.49298394\n",
      "nov 0.49039885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date 0.463761\n",
      "dec 0.45461416\n",
      "monday 0.43920517\n",
      "dodge 0.43715194\n",
      "sunday 0.42820418\n",
      "wheel 0.42705303\n",
      "aug 0.41943446\n",
      "editorial 0.41925302\n",
      "magazine 0.4116229\n",
      "modified 0.4029465\n",
      "friday 0.40055007\n",
      "driver 0.40027285\n",
      "daily 0.40002313\n",
      "anniversary 0.3964363\n",
      "till 0.3925323\n",
      "wheels 0.39240223\n",
      "updated 0.38474593\n",
      "bulletin 0.3843024\n",
      "edition 0.38062045\n",
      "sometime 0.37765303\n",
      "obituary 0.3768561\n",
      "hatch 0.37548062\n",
      "reprint 0.37387925\n",
      "rightmost 0.37323454\n",
      "wednesday 0.37283412\n",
      "cab 0.3722963\n",
      "window 0.37100628\n",
      "driveway 0.36807203\n",
      "manila 0.36623812\n",
      "unaltered 0.3654454\n",
      "copy 0.3644751\n",
      "streets 0.36131188\n",
      "sidewalk 0.3551067\n",
      "windows 0.35484365\n",
      "release 0.350844\n",
      "maj 0.35044038\n",
      "moon 0.3463262\n",
      "ballot 0.34542784\n",
      "bent 0.34320325\n",
      "midnight 0.3424678\n",
      "generic 0.3398788\n",
      "corners 0.33692536\n",
      "oregonian 0.3357302\n",
      "sheet 0.3338419\n",
      "bar 0.3327724\n",
      "door 0.33205172\n",
      "framed 0.33196595\n",
      "box 0.33149624\n",
      "newspapers 0.33031338\n",
      "piece 0.32881314\n",
      "jan 0.32856736\n",
      "occupants 0.3285075\n",
      "commemorative 0.32812667\n",
      "pages 0.3268861\n",
      "domingo 0.32544926\n",
      "proceedings 0.3242795\n",
      "contents 0.32361096\n",
      "roadway 0.32177687\n",
      "clipped 0.32073933\n",
      "civic 0.3177283\n",
      "diary 0.31726608\n",
      "exterior 0.31541482\n",
      "storefront 0.31351197\n",
      "yorker 0.31232843\n",
      "statehood 0.30930412\n",
      "footage 0.3082303\n",
      "gen 0.30724633\n",
      "referendum 0.30704987\n",
      "header 0.30685976\n",
      "weekly 0.30659875\n",
      "grill 0.30421653\n",
      "\n",
      "\n",
      "----------------Expert: 5 ---------------------\n",
      "nrhp 0.9185019\n",
      "ice 0.9117302\n",
      "hockey 0.89087516\n",
      "nhl 0.8423376\n",
      "skating 0.7671162\n",
      "rink 0.7570413\n",
      "rinks 0.6171352\n",
      "lacrosse 0.575881\n",
      "york 0.56900156\n",
      "bruins 0.5425985\n",
      "skaters 0.50272983\n",
      "skate 0.4948675\n",
      "research 0.488902\n",
      "pittsburgh 0.48738372\n",
      "sciences 0.4723218\n",
      "manhattan 0.4539042\n",
      "frozen 0.4443803\n",
      "ottawa 0.44220972\n",
      "glacier 0.4347684\n",
      "philadelphia 0.43239045\n",
      "montreal 0.4319587\n",
      "roxbury 0.4236948\n",
      "toronto 0.4211316\n",
      "nyc 0.4164075\n",
      "cream 0.40769687\n",
      "westchester 0.39806315\n",
      "habs 0.39689156\n",
      "catholic 0.39507017\n",
      "polar 0.39379475\n",
      "curling 0.38373646\n",
      "handball 0.38083738\n",
      "brooklyn 0.37872896\n",
      "syracuse 0.3742039\n",
      "rochester 0.3741566\n",
      "massachusetts 0.37406245\n",
      "counseling 0.37367412\n",
      "winnipeg 0.3701167\n",
      "religions 0.36951867\n",
      "chicago 0.36852852\n",
      "bronx 0.36497712\n",
      "arctic 0.36183107\n",
      "catholics 0.35904655\n",
      "scientific 0.35878438\n",
      "billiard 0.35646117\n",
      "gymnastics 0.35178828\n",
      "jewish 0.35022795\n",
      "snow 0.3456227\n",
      "forensic 0.33535787\n",
      "disciplines 0.33518144\n",
      "priest 0.33282208\n",
      "glacial 0.33181772\n",
      "cup 0.33036953\n",
      "missionary 0.32965654\n",
      "zone 0.32631615\n",
      "yonge 0.3261975\n",
      "vancouver 0.32197604\n",
      "maple 0.32105243\n",
      "scientists 0.3207618\n",
      "researchers 0.32028\n",
      "pharmacology 0.31928796\n",
      "oceanography 0.31862116\n",
      "newark 0.3184284\n",
      "avalanche 0.31835258\n",
      "scientist 0.31739497\n",
      "religion 0.31384718\n",
      "scoreboard 0.31233153\n",
      "christianity 0.31132594\n",
      "quebec 0.31055352\n",
      "beer 0.31031817\n",
      "erie 0.31026253\n",
      "minnesota 0.3093873\n",
      "minneapolis 0.30834976\n",
      "russian 0.30798656\n",
      "caps 0.30708128\n",
      "estonian 0.30674696\n",
      "leagues 0.3065258\n",
      "orthodox 0.30448326\n",
      "canada 0.3040225\n",
      "niagara 0.30313656\n",
      "volleyball 0.30304158\n",
      "penguin 0.30204403\n",
      "archdiocese 0.30159026\n",
      "edmonton 0.30067307\n",
      "rabbi 0.2979364\n",
      "biomedical 0.2977233\n",
      "emeritus 0.29707086\n",
      "nutrition 0.29640839\n",
      "tampa 0.2962577\n",
      "milwaukee 0.29617274\n",
      "bleacher 0.2958693\n",
      "biological 0.29561418\n",
      "ontario 0.29367822\n",
      "indianapolis 0.29367012\n",
      "swimming 0.29252127\n",
      "continents 0.29181734\n",
      "indoor 0.29039487\n",
      "christians 0.2900911\n",
      "chocolate 0.28998023\n",
      "maker 0.2897599\n",
      "stadiums 0.28840286\n",
      "\n",
      "\n",
      "----------------Expert: 6 ---------------------\n",
      "station 0.7454726\n",
      "stations 0.5880854\n",
      "bus 0.4480553\n",
      "first 0.4162493\n",
      "second 0.4091279\n",
      "service 0.39442766\n",
      "buses 0.37784863\n",
      "third 0.37362084\n",
      "terminal 0.36969024\n",
      "tram 0.35714057\n",
      "transit 0.3564608\n",
      "sanitation 0.34569827\n",
      "train 0.34521505\n",
      "railway 0.34344953\n",
      "mansfield 0.34341037\n",
      "services 0.33634233\n",
      "company 0.3256808\n",
      "fourth 0.3216705\n",
      "radio 0.3207665\n",
      "delivery 0.3200881\n",
      "evacuation 0.31754932\n",
      "rail 0.3137562\n",
      "tramway 0.3113395\n",
      "satellite 0.3110384\n",
      "parc 0.3110019\n",
      "romania 0.31012613\n",
      "alarm 0.30680215\n",
      "companies 0.30608356\n",
      "crews 0.3047282\n",
      "subway 0.30329794\n",
      "network 0.29419398\n",
      "meter 0.2937436\n",
      "cars 0.29216993\n",
      "routes 0.2912217\n",
      "carrier 0.28951678\n",
      "rental 0.28641862\n",
      "commuter 0.28627107\n",
      "freight 0.28604424\n",
      "cladding 0.28267756\n",
      "ukraine 0.2822774\n",
      "vehicles 0.28170788\n",
      "passenger 0.28140232\n",
      "porta 0.2802413\n",
      "fleet 0.27989995\n",
      "route 0.27962622\n",
      "baltic 0.27841142\n",
      "launch 0.2776409\n",
      "hour 0.27660504\n",
      "vehicle 0.27660006\n",
      "port 0.27578843\n",
      "fastest 0.27569178\n",
      "signal 0.27552065\n",
      "trains 0.2753637\n",
      "operators 0.2747836\n",
      "park 0.27436435\n",
      "azerbaijan 0.27314323\n",
      "regeneration 0.27241015\n",
      "transport 0.27042767\n",
      "submarine 0.26934287\n",
      "deliveries 0.26859733\n",
      "railroads 0.26618558\n",
      "short 0.26596794\n",
      "minute 0.265853\n",
      "cable 0.26546815\n",
      "johannesburg 0.26499373\n",
      "three 0.26492575\n",
      "gauge 0.26446348\n",
      "kiosk 0.2642517\n",
      "passengers 0.26319727\n",
      "terrestrial 0.2625694\n",
      "slovakia 0.2621385\n",
      "operator 0.26132694\n",
      "shower 0.26065892\n",
      "crossings 0.25967714\n",
      "streetcar 0.25956964\n",
      "signals 0.25949132\n",
      "caucasus 0.2594367\n",
      "nottinghamshire 0.25867885\n",
      "contract 0.2581657\n",
      "commercial 0.2576689\n",
      "connection 0.2571568\n",
      "estonia 0.2571116\n",
      "trucks 0.2568012\n",
      "tender 0.25636613\n",
      "television 0.2560653\n",
      "single 0.25560606\n",
      "gate 0.2555466\n",
      "vehicular 0.25533822\n",
      "warsaw 0.2550385\n",
      "car 0.2546799\n",
      "cabins 0.25428116\n",
      "space 0.25422737\n",
      "transportation 0.25400585\n",
      "showers 0.2537146\n",
      "villagers 0.25349885\n",
      "ferry 0.25343007\n",
      "congestion 0.2528675\n",
      "ten 0.25255278\n",
      "venture 0.25182518\n",
      "one 0.25154468\n",
      "\n",
      "\n",
      "----------------Expert: 7 ---------------------\n",
      "state 0.94457924\n",
      "federal 0.92307496\n",
      "county 0.9084787\n",
      "courthouse 0.85575414\n",
      "government 0.8531446\n",
      "city 0.8514034\n",
      "national 0.8435464\n",
      "council 0.84190464\n",
      "court 0.8105446\n",
      "states 0.7982707\n",
      "local 0.7571611\n",
      "town 0.74152553\n",
      "office 0.7289033\n",
      "capitol 0.71755916\n",
      "legislature 0.6897801\n",
      "municipal 0.6698389\n",
      "commissioner 0.6313978\n",
      "gov 0.61068773\n",
      "courts 0.6049751\n",
      "headquarters 0.59687597\n",
      "commissioners 0.58448625\n",
      "sheriff 0.58435535\n",
      "officials 0.5659218\n",
      "legislative 0.5639808\n",
      "attorney 0.5565949\n",
      "offices 0.550203\n",
      "mayors 0.54876983\n",
      "ruling 0.53643125\n",
      "police 0.52851826\n",
      "deputy 0.5268129\n",
      "provincial 0.5211851\n",
      "united 0.5128886\n",
      "citizens 0.50961846\n",
      "mayor 0.5011634\n",
      "law 0.49873346\n",
      "govt 0.4886232\n",
      "statewide 0.48598966\n",
      "governments 0.48124766\n",
      "union 0.4768157\n",
      "jail 0.47311628\n",
      "courthouses 0.4639227\n",
      "authorities 0.46340677\n",
      "authority 0.46115044\n",
      "congressional 0.45924988\n",
      "president 0.45680797\n",
      "cities 0.45630872\n",
      "secretary 0.4523578\n",
      "regional 0.44874057\n",
      "spokesman 0.4426578\n",
      "magistrate 0.4400832\n",
      "councils 0.44007632\n",
      "deputies 0.43905273\n",
      "judge 0.43510845\n",
      "enforcement 0.43431032\n",
      "petition 0.433601\n",
      "courtroom 0.43293872\n",
      "chief 0.43079272\n",
      "counties 0.42743856\n",
      "official 0.42058298\n",
      "nation 0.41395092\n",
      "executive 0.40995327\n",
      "clerk 0.4081891\n",
      "proclamation 0.4035439\n",
      "jurisdiction 0.40270302\n",
      "elections 0.39904428\n",
      "georgia 0.39860567\n",
      "commonwealth 0.3986027\n",
      "laws 0.39434275\n",
      "attorneys 0.39251184\n",
      "legislation 0.38724807\n",
      "counsel 0.3858366\n",
      "mississippi 0.3855899\n",
      "delegation 0.38400322\n",
      "texas 0.38369656\n",
      "representative 0.38242134\n",
      "convention 0.37858078\n",
      "chairman 0.3747091\n",
      "municipality 0.37394786\n",
      "towns 0.3724716\n",
      "metropolitan 0.36937454\n",
      "arkansas 0.36590725\n",
      "judiciary 0.36534405\n",
      "congress 0.36491925\n",
      "lawyer 0.36473042\n",
      "polk 0.35904416\n",
      "representatives 0.3572603\n",
      "citizen 0.35691822\n",
      "missouri 0.35484824\n",
      "tennessee 0.35469705\n",
      "governmental 0.35315928\n",
      "coalition 0.35189658\n",
      "custody 0.35142103\n",
      "cuyahoga 0.3490995\n",
      "senate 0.34715238\n",
      "highways 0.3458646\n",
      "fairgrounds 0.34357652\n",
      "tax 0.34327194\n",
      "vice 0.34300512\n",
      "judicial 0.34168696\n",
      "hometown 0.3410625\n",
      "\n",
      "\n",
      "----------------Expert: 8 ---------------------\n",
      "east 0.9229543\n",
      "west 0.9157393\n",
      "north 0.90461326\n",
      "south 0.9042979\n",
      "river 0.76372194\n",
      "northeast 0.75620073\n",
      "northeast 0.75620073\n",
      "northwest 0.74960065\n",
      "northwest 0.74960065\n",
      "southeast 0.72852045\n",
      "southeast 0.72852045\n",
      "bank 0.6773667\n",
      "southwest 0.6649269\n",
      "southwest 0.6649269\n",
      "central 0.6550817\n",
      "district 0.6242709\n",
      "junction 0.61314946\n",
      "eastern 0.56778425\n",
      "districts 0.5624349\n",
      "confluence 0.556867\n",
      "banks 0.5471697\n",
      "basin 0.5239825\n",
      "northern 0.503147\n",
      "southern 0.49810025\n",
      "rivers 0.48887333\n",
      "southward 0.48706493\n",
      "tributary 0.48275107\n",
      "situated 0.46619344\n",
      "area 0.46364713\n",
      "exit 0.46019867\n",
      "valley 0.44021395\n",
      "historic 0.4337826\n",
      "estuary 0.4299328\n",
      "intersection 0.42885098\n",
      "plains 0.4252674\n",
      "lender 0.42262605\n",
      "coast 0.42160234\n",
      "located 0.41349396\n",
      "northwestern 0.40998596\n",
      "northwestern 0.40998596\n",
      "kilometres 0.40906534\n",
      "region 0.40853116\n",
      "crossing 0.40765154\n",
      "foreclosure 0.40742686\n",
      "frontage 0.4062178\n",
      "neighborhoods 0.3985087\n",
      "lake 0.3948683\n",
      "western 0.3946071\n",
      "southeastern 0.39161804\n",
      "southeastern 0.39161804\n",
      "miles 0.39086148\n",
      "coastal 0.39039618\n",
      "westward 0.38785082\n",
      "avenue 0.3768629\n",
      "boundary 0.37416106\n",
      "creek 0.37216043\n",
      "northwards 0.36968246\n",
      "northeastern 0.36830416\n",
      "northeastern 0.36830416\n",
      "inland 0.36735722\n",
      "canal 0.36396706\n",
      "reserve 0.36271352\n",
      "lowlands 0.3547687\n",
      "downstream 0.35371855\n",
      "kilometers 0.35363078\n",
      "hamlet 0.35318342\n",
      "meridian 0.3505395\n",
      "reservoir 0.34998897\n",
      "terminus 0.345599\n",
      "near 0.34441015\n",
      "peninsula 0.34272572\n",
      "riverside 0.33516535\n",
      "townships 0.33450186\n",
      "bluff 0.33339033\n",
      "marsh 0.33321136\n",
      "metres 0.33069345\n",
      "township 0.32883364\n",
      "mortgages 0.32827917\n",
      "ridge 0.32313985\n",
      "rapids 0.31803435\n",
      "ave 0.3177923\n",
      "southwestern 0.31618828\n",
      "southwestern 0.31618828\n",
      "suburb 0.31542143\n",
      "investors 0.31524011\n",
      "upstream 0.31494915\n",
      "delta 0.31102538\n",
      "flood 0.31075618\n",
      "areas 0.31030348\n",
      "paddington 0.30958983\n",
      "scenic 0.30817205\n",
      "foothills 0.30487874\n",
      "plateau 0.30451524\n",
      "fertile 0.30292907\n",
      "flooding 0.30109358\n",
      "suburbs 0.3009237\n",
      "crossroads 0.29988438\n",
      "deposits 0.29871634\n",
      "creeks 0.29836103\n",
      "hampstead 0.29751775\n",
      "\n",
      "\n",
      "----------------Expert: 9 ---------------------\n",
      "gallery 0.9279226\n",
      "collections 0.91784686\n",
      "museum 0.9105724\n",
      "galleries 0.8930947\n",
      "art 0.8926729\n",
      "library 0.88684505\n",
      "collection 0.8819084\n",
      "artists 0.87774765\n",
      "exhibition 0.8441443\n",
      "artist 0.833217\n",
      "paintings 0.8028089\n",
      "site 0.75823206\n",
      "artworks 0.75815296\n",
      "exhibitions 0.7455481\n",
      "books 0.73923004\n",
      "museums 0.7266891\n",
      "buildings 0.6836525\n",
      "curator 0.67903817\n",
      "libraries 0.6706573\n",
      "painting 0.6583236\n",
      "building 0.64780235\n",
      "sculpture 0.635517\n",
      "artwork 0.61677074\n",
      "sculptures 0.6152341\n",
      "drawings 0.6099805\n",
      "works 0.59357995\n",
      "exhibit 0.58508974\n",
      "catalogue 0.5792834\n",
      "website 0.5715944\n",
      "project 0.56981224\n",
      "book 0.5456969\n",
      "floor 0.51713663\n",
      "archive 0.5147812\n",
      "artifacts 0.51174206\n",
      "projects 0.4918733\n",
      "albums 0.48443538\n",
      "masterpieces 0.48169243\n",
      "objects 0.4651111\n",
      "pottery 0.4582608\n",
      "collectors 0.45675087\n",
      "sketches 0.4452431\n",
      "archives 0.4388151\n",
      "catalog 0.43581423\n",
      "exhibits 0.4346706\n",
      "photographs 0.43330598\n",
      "archival 0.42462698\n",
      "web 0.41901475\n",
      "paint 0.41672453\n",
      "collector 0.41427067\n",
      "graffiti 0.4135043\n",
      "database 0.4123471\n",
      "history 0.41161305\n",
      "bookstore 0.40318188\n",
      "sites 0.39931893\n",
      "mosaics 0.39877042\n",
      "memorabilia 0.39837447\n",
      "drawing 0.39567873\n",
      "treasures 0.39402628\n",
      "prints 0.39256212\n",
      "databases 0.3893396\n",
      "repository 0.38889492\n",
      "novels 0.38470194\n",
      "sculptors 0.38379958\n",
      "murals 0.38324085\n",
      "publications 0.3823818\n",
      "watercolor 0.3819065\n",
      "creations 0.3813582\n",
      "sculptor 0.38129354\n",
      "antiquities 0.3812673\n",
      "artistic 0.3798075\n",
      "artefacts 0.37896723\n",
      "antiques 0.36737785\n",
      "author 0.36619106\n",
      "literary 0.36588234\n",
      "mural 0.36126417\n",
      "canvas 0.35758635\n",
      "frescoes 0.35427612\n",
      "illustrations 0.3524254\n",
      "stories 0.3518431\n",
      "webpage 0.35089463\n",
      "work 0.34865722\n",
      "manuscripts 0.34849507\n",
      "copies 0.34231055\n",
      "page 0.3378381\n",
      "build 0.3342027\n",
      "poetry 0.33367214\n",
      "furniture 0.33221644\n",
      "walls 0.32699668\n",
      "painters 0.32202262\n",
      "crafts 0.32076246\n",
      "materials 0.31884784\n",
      "ceramics 0.31769574\n",
      "holdings 0.31404653\n",
      "preservation 0.3122007\n",
      "periodicals 0.31148827\n",
      "images 0.3103756\n",
      "writing 0.30875832\n",
      "historian 0.30828363\n",
      "composers 0.3061953\n",
      "illustrated 0.30611965\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#we traind it for 3 round here for simplicity\n",
    "\n",
    "for moe in range(3):\n",
    "    print('--------------------------ROUND:',moe,'--------------------------')\n",
    "    models=[]\n",
    "    experts=10\n",
    "    g_k_J=np.transpose(g_k_J)\n",
    "    for k in range(experts):\n",
    "        \n",
    "       #self, count_,entities,fea,embedding_size, entities_size,g_K_j,empEnt,empFea, entbias,feabias,max_vocab_size=100000, min_occurrences=0,\n",
    "   \n",
    "        models.append(GloVeModelMOE(cooccurrence_matrix,fea,ent,np.array(g_k_J[k]),np.array(empFea[k]),\n",
    "                                    np.array(empEnt[k]),np.array(feabias[k]),np.array(entbias[k]),\n",
    "                                    embedding_size=10, entities_size=len(ent), min_occurrences=0,\n",
    "                               learning_rate=0.5, batch_size=1000))\n",
    "    empEntTemp=[]\n",
    "    empFeaTemp=[]\n",
    "    errFeaTemp=[]\n",
    "    entbiasTemp=[]\n",
    "    feabiasTemp=[]\n",
    "    for j in range(experts):\n",
    "        print('Starting Expert:', j)\n",
    "        models[j].fit_to_corpus(cooccurrence_matrix)\n",
    "        models[j].train(num_epochs=10, log_dir=\"log/example\", summary_batch_interval=100)\n",
    "        feabiasTemp.append(models[j].context_biasesE)\n",
    "        entbiasTemp.append(models[j].focal_biasesW)\n",
    "        empFeaTemp.append(models[j].embeddings_en)\n",
    "        empEntTemp.append(models[j].embeddings)\n",
    "        errFeaTemp.append(models[j].errorEstimation)\n",
    "    empEnt=np.array(empEntTemp)\n",
    "    empFea=np.array(empFeaTemp)\n",
    "    errFea=np.array(errFeaTemp)\n",
    "    entbias=np.array(entbiasTemp)\n",
    "    feabias=np.array(feabiasTemp)\n",
    "    scores=E_step(errFea)#E_step()#\n",
    "    print(np.array(scores).shape)\n",
    "    if (moe)<3:\n",
    "        g_k_J,loss=moeGate((scores),gloveVectors)\n",
    "        plt.plot(loss)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# In[274]:\n",
    "print('--------------------------------------Here What Iam Looking For--------------------------------------')\n",
    "\n",
    "print(np.array(g_k_J).shape)\n",
    "g_k_J=np.transpose(g_k_J)\n",
    "for k in range(experts):\n",
    "    print()\n",
    "    print()\n",
    "    print('----------------Expert:',k,'---------------------')\n",
    "    count=0\n",
    "    for i in np.argsort(np.array(g_k_J)[k])[::-1]:\n",
    "        \n",
    "        if count<100 and k==np.argmax(g_k_J[:,i]):\n",
    "            count=count+1\n",
    "            print(fea[i],g_k_J[k][i])\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "# In[302]:\n",
    "\n",
    "\n",
    "ex=[[],[],[],[],[],[],[],[],[],[]]\n",
    "exV=[[],[],[],[],[],[],[],[],[],[]]\n",
    "exscore=[[],[],[],[],[],[],[],[],[],[]]\n",
    "#print('Here an Erorr',len(fea),np.array(g_k_J).shape)\n",
    "\n",
    "for i in range(len(fea)):\n",
    "    x=np.argmax(g_k_J[:,i])\n",
    "    if g_k_J[x][i]<=1 and g_k_J[x][i]>=.1:\n",
    "        exscore[x].append(g_k_J[x][i])\n",
    "        ex[x].append(fea[i])#features for each expert\n",
    "        exV[x].append(empFea[x][i])#features vectors\n",
    "        \n",
    "        \n",
    "\n",
    "# In[9]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3721, 10)\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_0\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_0\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_0\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_0\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_1\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_1\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_1\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_1\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_2\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_2\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_2\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_2\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_3\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_3\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_3\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_3\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_4\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_4\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_4\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_4\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_5\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_5\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_5\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_5\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_6\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_6\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_6\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_6\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_7\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_7\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_7\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_7\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_8\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_8\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_8\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_8\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ent_for_expert_9\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/fea_for_expert_9\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/exV_fea_for_expert_9\n",
      "starting array\n",
      "successful write //media/rana/2TB/MoECode/results/ex_fea_for_expert_9\n"
     ]
    }
   ],
   "source": [
    "#saving the entity embedings, features, and features directions for each facet in a seperate file\n",
    "\n",
    "print(np.array(np.array(empEnt)).shape)\n",
    "orderd_features=fea\n",
    "dim_5_all=np.array(empEnt) # entity embeddings for each experts \n",
    "featuersDirection_for_each_facet=np.array(empFea)\n",
    "try:\n",
    "    '''write2dArray(mdsMAIN,base_folder_results+'GloveBaseline')\n",
    "    write2dArray(orderd_features_directions,base_folder_results+'GloveBaselineOrderd_features_directions')'''\n",
    "    for i in range(experts):\n",
    "\n",
    "        write2dArray(dim_5_all[i],base_folder_results+'ent_for_expert_'+str(i))\n",
    "        write2dArray(featuersDirection_for_each_facet[i],base_folder_results+'fea_for_expert_'+str(i))\n",
    "        write2dArray(exV[i],base_folder_results+'exV_fea_for_expert_'+str(i))\n",
    "        write1dArray(ex[i],base_folder_results+'ex_fea_for_expert_'+str(i))\n",
    "except:\n",
    "    print('didnt save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
